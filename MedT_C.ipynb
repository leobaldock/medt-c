{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6767208c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 1.8.1\n",
      "\n",
      "Python 3.8.8 (default, Feb 24 2021, 21:46:12) \n",
      "[GCC 7.3.0]\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# from AxialAttention_dynamic import AxialAttention_dynamic as AxialAttention\n",
    "\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torchsummary import summary\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(\"GPU is\", \"available\" if torch.cuda.is_available() else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385f9ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f42e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58dd7caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AxialAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads, is_width=False):\n",
    "        super(AxialAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "        self.is_width = is_width\n",
    "        \n",
    "        assert (self.head_dim * heads == embed_size), \"Embed size needs to be divisible by heads\"\n",
    "        \n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads*self.head_dim, embed_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.is_width:\n",
    "            y = x.permute(0, 2, 3, 1)\n",
    "        else:\n",
    "            y = x.permute(0, 3, 2, 1)\n",
    "        N, D1, D2, C = y.shape\n",
    "        y = y.contiguous().view(N * D1, D2, C)\n",
    "        \n",
    "        # Split input into self.heads chunks. possible that this needs to go after the linear projections\n",
    "        y = y.reshape(\n",
    "                N * D1,\n",
    "                D2,\n",
    "                self.heads, \n",
    "                self.head_dim\n",
    "            )\n",
    "        \n",
    "        values = self.values(y)\n",
    "        keys = self.keys(y)\n",
    "        queries = self.queries(y)\n",
    "        \n",
    "        #this is the QK matrix multiply step\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        # queries shape: (N, query_len, heads, heads_dim)\n",
    "        # keys shape: (N, key_len, heads, heads_dim)\n",
    "        # energy shape: (N, heads, query_len, key_len)\n",
    "            \n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1/2)), dim=3)\n",
    "        \n",
    "        # concat the heads with wierd reshape thing\n",
    "        y = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, D1, D2, C\n",
    "        )\n",
    "        # attention shape: (N, heads, query_len, key_len)\n",
    "        # values shape: (N, value_len, heads, heads_dim)\n",
    "        # after einsum: (N, query_len, heads, head_dim) then flatten last two dimensions\n",
    "        \n",
    "        y = self.fc_out(y)\n",
    "        if self.is_width:\n",
    "            y = y.permute(0, 3, 1, 2)\n",
    "        else:\n",
    "            y = y.permute(0, 3, 2, 1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86428e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedAxialTransformerLayer(nn.Module):\n",
    "    def __init__(self, in_channels, heads):\n",
    "        super(GatedAxialTransformerLayer, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.attention_channels = in_channels // 2 # downsample by 50%\n",
    "        self.heads = heads\n",
    "        \n",
    "        # Conv 1x1 to halve size\n",
    "        self.conv_down = nn.Conv2d(self.in_channels, self.attention_channels, kernel_size=1, stride=1)\n",
    "        \n",
    "        # Norm\n",
    "        self.bn1 = nn.BatchNorm2d(self.attention_channels)\n",
    "        \n",
    "        # GatedAxialAttention height\n",
    "        self.height_attention = AxialAttention(self.attention_channels, heads)\n",
    "        \n",
    "        # GatedAxialAttention width\n",
    "        self.width_attention = AxialAttention(self.attention_channels, heads, is_width=True)\n",
    "        \n",
    "        # Conv 1x1 to increase size\n",
    "        self.conv_up = nn.Conv2d(self.attention_channels, in_channels, kernel_size=1, stride=1)\n",
    "        \n",
    "        # Norm\n",
    "        self.bn2 = nn.BatchNorm2d(self.in_channels)\n",
    "        \n",
    "        # ReLU activation\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Conv 1x1 to halve size\n",
    "        y = self.conv_down(x)\n",
    "        \n",
    "        # Norm\n",
    "        y = self.bn1(y)\n",
    "        \n",
    "        # ReLU\n",
    "        y = self.relu(y)\n",
    "        \n",
    "        # GatedAxialAttention height\n",
    "        y = self.height_attention(y)\n",
    "        \n",
    "        # GatedAxialAttention width\n",
    "        y = self.width_attention(y)\n",
    "        \n",
    "        # ReLU\n",
    "        y = self.relu(y)\n",
    "        \n",
    "        # Conv 1x1 to increase size\n",
    "        y = self.conv_up(y)\n",
    "        \n",
    "        # Norm\n",
    "        y = self.bn2(y)\n",
    "        \n",
    "        # Residual connection addition from input\n",
    "        y += x\n",
    "        \n",
    "        # ReLU\n",
    "        y = self.relu(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a2cd5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_channels,\n",
    "        num_transformer_layers,\n",
    "        color_channels=3,\n",
    "        conv_kernel_size=7,\n",
    "        heads=8,\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                # TODO: abstract this, including padding number, see ResNet file\n",
    "                nn.Conv2d(\n",
    "                    in_channels=color_channels,\n",
    "                    out_channels=feature_channels,\n",
    "                    kernel_size=conv_kernel_size,\n",
    "                    stride=1,\n",
    "                    padding=3\n",
    "                )\n",
    "            ] +\n",
    "            [\n",
    "                GatedAxialTransformerLayer(in_channels=feature_channels, heads=heads) \n",
    "                for _ in range(num_transformer_layers)\n",
    "            ],\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            y = layer(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0075ddea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.avg = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.decoder = nn.Linear(in_channels, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.avg(x)\n",
    "        y = y.view(y.size(0), -1)\n",
    "        y = self.decoder(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7ea5963a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedT_C(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_dim=32,\n",
    "        in_channels=3,\n",
    "        patch_dim=8,\n",
    "        num_classes=10,\n",
    "        feature_dim=256,\n",
    "    ):\n",
    "        super(MedT_C, self).__init__()\n",
    "        assert img_dim % patch_dim == 0, f\"Image dimension {img_dim} is not divisible by patch dimension {patch_dim}.\"\n",
    "        self.img_dim = img_dim\n",
    "        self.patch_dim = patch_dim\n",
    "        \n",
    "        self.global_branch = Encoder(feature_dim, 2)\n",
    "        self.local_branch = Encoder(feature_dim, 5)\n",
    "\n",
    "        self.decoder = Decoder(feature_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        yg = self.global_branch(x)\n",
    "        # could probably do this with convolution and then reshape, would likely be faster...\n",
    "        yl = yg.clone()\n",
    "        for i in range(0,self.img_dim // self.patch_dim):\n",
    "            for j in range(0,self.img_dim // self.patch_dim):\n",
    "                patch = x[:,:,self.patch_dim*i:self.patch_dim*(i+1),self.patch_dim*j:self.patch_dim*(j+1)]\n",
    "                y_patch = self.local_branch(patch)\n",
    "                yl[:,:,self.patch_dim*i:self.patch_dim*(i+1),self.patch_dim*j:self.patch_dim*(j+1)] = y_patch\n",
    "        \n",
    "        y = yg + yl # not sure if this is the right way to do a summation?\n",
    "        y = self.decoder(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9436de84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "imshow(images[0])\n",
    "images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "47e852a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YLshape: torch.Size([1, 256, 32, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MedT_C(\n",
    "    img_dim=32,\n",
    "    in_channels=3,\n",
    "    patch_dim=8,\n",
    "    num_classes=10,\n",
    "    feature_dim=256\n",
    ")\n",
    "\n",
    "x = images[0].unsqueeze(0)\n",
    "out = model(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "05fa6b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MedT_C(\n",
      "  (global_branch): Encoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): Conv2d(3, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "      (1): GatedAxialTransformerLayer(\n",
      "        (conv_down): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (height_attention): AxialAttention(\n",
      "          (values): Linear(in_features=16, out_features=16, bias=False)\n",
      "          (keys): Linear(in_features=16, out_features=16, bias=False)\n",
      "          (queries): Linear(in_features=16, out_features=16, bias=False)\n",
      "          (fc_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (width_attention): AxialAttention(\n",
      "          (values): Linear(in_features=16, out_features=16, bias=False)\n",
      "          (keys): Linear(in_features=16, out_features=16, bias=False)\n",
      "          (queries): Linear(in_features=16, out_features=16, bias=False)\n",
      "          (fc_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (conv_up): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): GatedAxialTransformerLayer(\n",
      "        (conv_down): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (height_attention): AxialAttention(\n",
      "          (values): Linear(in_features=16, out_features=16, bias=False)\n",
      "          (keys): Linear(in_features=16, out_features=16, bias=False)\n",
      "          (queries): Linear(in_features=16, out_features=16, bias=False)\n",
      "          (fc_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (width_attention): AxialAttention(\n",
      "          (values): Linear(in_features=16, out_features=16, bias=False)\n",
      "          (keys): Linear(in_features=16, out_features=16, bias=False)\n",
      "          (queries): Linear(in_features=16, out_features=16, bias=False)\n",
      "          (fc_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (conv_up): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (local_branch): Encoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): Conv2d(3, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "      (1): GatedAxialTransformerLayer(\n",
      "        (conv_down): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (height_attention): AxialAttention(\n",
      "          (values): Linear(in_features=16, out_features=16, bias=False)\n",
      "          (keys): Linear(in_features=16, out_features=16, bias=False)\n",
      "          (queries): Linear(in_features=16, out_features=16, bias=False)\n",
      "          (fc_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (width_attention): AxialAttention(\n",
      "          (values): Linear(in_features=16, out_features=16, bias=False)\n",
      "          (keys): Linear(in_features=16, out_features=16, bias=False)\n",
      "          (queries): Linear(in_features=16, out_features=16, bias=False)\n",
      "          (fc_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (conv_up): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): GatedAxialTransformerLayer(\n",
      "        (conv_down): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (height_attention): AxialAttention(\n",
      "          (values): Linear(in_features=16, out_features=16, bias=False)\n",
      "          (keys): Linear(in_features=16, out_features=16, bias=False)\n",
      "          (queries): Linear(in_features=16, out_features=16, bias=False)\n",
      "          (fc_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (width_attention): AxialAttention(\n",
      "          (values): Linear(in_features=16, out_features=16, bias=False)\n",
      "          (keys): Linear(in_features=16, out_features=16, bias=False)\n",
      "          (queries): Linear(in_features=16, out_features=16, bias=False)\n",
      "          (fc_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (conv_up): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): GatedAxialTransformerLayer(\n",
      "        (conv_down): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (height_attention): AxialAttention(\n",
      "          (values): Linear(in_features=16, out_features=16, bias=False)\n",
      "          (keys): Linear(in_features=16, out_features=16, bias=False)\n",
      "          (queries): Linear(in_features=16, out_features=16, bias=False)\n",
      "          (fc_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (width_attention): AxialAttention(\n",
      "          (values): Linear(in_features=16, out_features=16, bias=False)\n",
      "          (keys): Linear(in_features=16, out_features=16, bias=False)\n",
      "          (queries): Linear(in_features=16, out_features=16, bias=False)\n",
      "          (fc_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (conv_up): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): GatedAxialTransformerLayer(\n",
      "        (conv_down): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (height_attention): AxialAttention(\n",
      "          (values): Linear(in_features=16, out_features=16, bias=False)\n",
      "          (keys): Linear(in_features=16, out_features=16, bias=False)\n",
      "          (queries): Linear(in_features=16, out_features=16, bias=False)\n",
      "          (fc_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (width_attention): AxialAttention(\n",
      "          (values): Linear(in_features=16, out_features=16, bias=False)\n",
      "          (keys): Linear(in_features=16, out_features=16, bias=False)\n",
      "          (queries): Linear(in_features=16, out_features=16, bias=False)\n",
      "          (fc_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (conv_up): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): GatedAxialTransformerLayer(\n",
      "        (conv_down): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (height_attention): AxialAttention(\n",
      "          (values): Linear(in_features=16, out_features=16, bias=False)\n",
      "          (keys): Linear(in_features=16, out_features=16, bias=False)\n",
      "          (queries): Linear(in_features=16, out_features=16, bias=False)\n",
      "          (fc_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (width_attention): AxialAttention(\n",
      "          (values): Linear(in_features=16, out_features=16, bias=False)\n",
      "          (keys): Linear(in_features=16, out_features=16, bias=False)\n",
      "          (queries): Linear(in_features=16, out_features=16, bias=False)\n",
      "          (fc_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (conv_up): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (avg): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (decoder): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "YLshape: torch.Size([2, 256, 32, 32])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 256, 32, 32]          37,888\n",
      "            Conv2d-2          [-1, 128, 32, 32]          32,896\n",
      "       BatchNorm2d-3          [-1, 128, 32, 32]             256\n",
      "              ReLU-4          [-1, 128, 32, 32]               0\n",
      "            Linear-5            [-1, 32, 8, 16]             256\n",
      "            Linear-6            [-1, 32, 8, 16]             256\n",
      "            Linear-7            [-1, 32, 8, 16]             256\n",
      "            Linear-8          [-1, 32, 32, 128]          16,512\n",
      "    AxialAttention-9          [-1, 128, 32, 32]               0\n",
      "           Linear-10            [-1, 32, 8, 16]             256\n",
      "           Linear-11            [-1, 32, 8, 16]             256\n",
      "           Linear-12            [-1, 32, 8, 16]             256\n",
      "           Linear-13          [-1, 32, 32, 128]          16,512\n",
      "   AxialAttention-14          [-1, 128, 32, 32]               0\n",
      "             ReLU-15          [-1, 128, 32, 32]               0\n",
      "           Conv2d-16          [-1, 256, 32, 32]          33,024\n",
      "      BatchNorm2d-17          [-1, 256, 32, 32]             512\n",
      "             ReLU-18          [-1, 256, 32, 32]               0\n",
      "GatedAxialTransformerLayer-19          [-1, 256, 32, 32]               0\n",
      "           Conv2d-20          [-1, 128, 32, 32]          32,896\n",
      "      BatchNorm2d-21          [-1, 128, 32, 32]             256\n",
      "             ReLU-22          [-1, 128, 32, 32]               0\n",
      "           Linear-23            [-1, 32, 8, 16]             256\n",
      "           Linear-24            [-1, 32, 8, 16]             256\n",
      "           Linear-25            [-1, 32, 8, 16]             256\n",
      "           Linear-26          [-1, 32, 32, 128]          16,512\n",
      "   AxialAttention-27          [-1, 128, 32, 32]               0\n",
      "           Linear-28            [-1, 32, 8, 16]             256\n",
      "           Linear-29            [-1, 32, 8, 16]             256\n",
      "           Linear-30            [-1, 32, 8, 16]             256\n",
      "           Linear-31          [-1, 32, 32, 128]          16,512\n",
      "   AxialAttention-32          [-1, 128, 32, 32]               0\n",
      "             ReLU-33          [-1, 128, 32, 32]               0\n",
      "           Conv2d-34          [-1, 256, 32, 32]          33,024\n",
      "      BatchNorm2d-35          [-1, 256, 32, 32]             512\n",
      "             ReLU-36          [-1, 256, 32, 32]               0\n",
      "GatedAxialTransformerLayer-37          [-1, 256, 32, 32]               0\n",
      "          Encoder-38          [-1, 256, 32, 32]               0\n",
      "           Conv2d-39            [-1, 256, 8, 8]          37,888\n",
      "           Conv2d-40            [-1, 128, 8, 8]          32,896\n",
      "      BatchNorm2d-41            [-1, 128, 8, 8]             256\n",
      "             ReLU-42            [-1, 128, 8, 8]               0\n",
      "           Linear-43             [-1, 8, 8, 16]             256\n",
      "           Linear-44             [-1, 8, 8, 16]             256\n",
      "           Linear-45             [-1, 8, 8, 16]             256\n",
      "           Linear-46            [-1, 8, 8, 128]          16,512\n",
      "   AxialAttention-47            [-1, 128, 8, 8]               0\n",
      "           Linear-48             [-1, 8, 8, 16]             256\n",
      "           Linear-49             [-1, 8, 8, 16]             256\n",
      "           Linear-50             [-1, 8, 8, 16]             256\n",
      "           Linear-51            [-1, 8, 8, 128]          16,512\n",
      "   AxialAttention-52            [-1, 128, 8, 8]               0\n",
      "             ReLU-53            [-1, 128, 8, 8]               0\n",
      "           Conv2d-54            [-1, 256, 8, 8]          33,024\n",
      "      BatchNorm2d-55            [-1, 256, 8, 8]             512\n",
      "             ReLU-56            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-57            [-1, 256, 8, 8]               0\n",
      "           Conv2d-58            [-1, 128, 8, 8]          32,896\n",
      "      BatchNorm2d-59            [-1, 128, 8, 8]             256\n",
      "             ReLU-60            [-1, 128, 8, 8]               0\n",
      "           Linear-61             [-1, 8, 8, 16]             256\n",
      "           Linear-62             [-1, 8, 8, 16]             256\n",
      "           Linear-63             [-1, 8, 8, 16]             256\n",
      "           Linear-64            [-1, 8, 8, 128]          16,512\n",
      "   AxialAttention-65            [-1, 128, 8, 8]               0\n",
      "           Linear-66             [-1, 8, 8, 16]             256\n",
      "           Linear-67             [-1, 8, 8, 16]             256\n",
      "           Linear-68             [-1, 8, 8, 16]             256\n",
      "           Linear-69            [-1, 8, 8, 128]          16,512\n",
      "   AxialAttention-70            [-1, 128, 8, 8]               0\n",
      "             ReLU-71            [-1, 128, 8, 8]               0\n",
      "           Conv2d-72            [-1, 256, 8, 8]          33,024\n",
      "      BatchNorm2d-73            [-1, 256, 8, 8]             512\n",
      "             ReLU-74            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-75            [-1, 256, 8, 8]               0\n",
      "           Conv2d-76            [-1, 128, 8, 8]          32,896\n",
      "      BatchNorm2d-77            [-1, 128, 8, 8]             256\n",
      "             ReLU-78            [-1, 128, 8, 8]               0\n",
      "           Linear-79             [-1, 8, 8, 16]             256\n",
      "           Linear-80             [-1, 8, 8, 16]             256\n",
      "           Linear-81             [-1, 8, 8, 16]             256\n",
      "           Linear-82            [-1, 8, 8, 128]          16,512\n",
      "   AxialAttention-83            [-1, 128, 8, 8]               0\n",
      "           Linear-84             [-1, 8, 8, 16]             256\n",
      "           Linear-85             [-1, 8, 8, 16]             256\n",
      "           Linear-86             [-1, 8, 8, 16]             256\n",
      "           Linear-87            [-1, 8, 8, 128]          16,512\n",
      "   AxialAttention-88            [-1, 128, 8, 8]               0\n",
      "             ReLU-89            [-1, 128, 8, 8]               0\n",
      "           Conv2d-90            [-1, 256, 8, 8]          33,024\n",
      "      BatchNorm2d-91            [-1, 256, 8, 8]             512\n",
      "             ReLU-92            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-93            [-1, 256, 8, 8]               0\n",
      "           Conv2d-94            [-1, 128, 8, 8]          32,896\n",
      "      BatchNorm2d-95            [-1, 128, 8, 8]             256\n",
      "             ReLU-96            [-1, 128, 8, 8]               0\n",
      "           Linear-97             [-1, 8, 8, 16]             256\n",
      "           Linear-98             [-1, 8, 8, 16]             256\n",
      "           Linear-99             [-1, 8, 8, 16]             256\n",
      "          Linear-100            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-101            [-1, 128, 8, 8]               0\n",
      "          Linear-102             [-1, 8, 8, 16]             256\n",
      "          Linear-103             [-1, 8, 8, 16]             256\n",
      "          Linear-104             [-1, 8, 8, 16]             256\n",
      "          Linear-105            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-106            [-1, 128, 8, 8]               0\n",
      "            ReLU-107            [-1, 128, 8, 8]               0\n",
      "          Conv2d-108            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-109            [-1, 256, 8, 8]             512\n",
      "            ReLU-110            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-111            [-1, 256, 8, 8]               0\n",
      "          Conv2d-112            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-113            [-1, 128, 8, 8]             256\n",
      "            ReLU-114            [-1, 128, 8, 8]               0\n",
      "          Linear-115             [-1, 8, 8, 16]             256\n",
      "          Linear-116             [-1, 8, 8, 16]             256\n",
      "          Linear-117             [-1, 8, 8, 16]             256\n",
      "          Linear-118            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-119            [-1, 128, 8, 8]               0\n",
      "          Linear-120             [-1, 8, 8, 16]             256\n",
      "          Linear-121             [-1, 8, 8, 16]             256\n",
      "          Linear-122             [-1, 8, 8, 16]             256\n",
      "          Linear-123            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-124            [-1, 128, 8, 8]               0\n",
      "            ReLU-125            [-1, 128, 8, 8]               0\n",
      "          Conv2d-126            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-127            [-1, 256, 8, 8]             512\n",
      "            ReLU-128            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-129            [-1, 256, 8, 8]               0\n",
      "         Encoder-130            [-1, 256, 8, 8]               0\n",
      "          Conv2d-131            [-1, 256, 8, 8]          37,888\n",
      "          Conv2d-132            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-133            [-1, 128, 8, 8]             256\n",
      "            ReLU-134            [-1, 128, 8, 8]               0\n",
      "          Linear-135             [-1, 8, 8, 16]             256\n",
      "          Linear-136             [-1, 8, 8, 16]             256\n",
      "          Linear-137             [-1, 8, 8, 16]             256\n",
      "          Linear-138            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-139            [-1, 128, 8, 8]               0\n",
      "          Linear-140             [-1, 8, 8, 16]             256\n",
      "          Linear-141             [-1, 8, 8, 16]             256\n",
      "          Linear-142             [-1, 8, 8, 16]             256\n",
      "          Linear-143            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-144            [-1, 128, 8, 8]               0\n",
      "            ReLU-145            [-1, 128, 8, 8]               0\n",
      "          Conv2d-146            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-147            [-1, 256, 8, 8]             512\n",
      "            ReLU-148            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-149            [-1, 256, 8, 8]               0\n",
      "          Conv2d-150            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-151            [-1, 128, 8, 8]             256\n",
      "            ReLU-152            [-1, 128, 8, 8]               0\n",
      "          Linear-153             [-1, 8, 8, 16]             256\n",
      "          Linear-154             [-1, 8, 8, 16]             256\n",
      "          Linear-155             [-1, 8, 8, 16]             256\n",
      "          Linear-156            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-157            [-1, 128, 8, 8]               0\n",
      "          Linear-158             [-1, 8, 8, 16]             256\n",
      "          Linear-159             [-1, 8, 8, 16]             256\n",
      "          Linear-160             [-1, 8, 8, 16]             256\n",
      "          Linear-161            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-162            [-1, 128, 8, 8]               0\n",
      "            ReLU-163            [-1, 128, 8, 8]               0\n",
      "          Conv2d-164            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-165            [-1, 256, 8, 8]             512\n",
      "            ReLU-166            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-167            [-1, 256, 8, 8]               0\n",
      "          Conv2d-168            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-169            [-1, 128, 8, 8]             256\n",
      "            ReLU-170            [-1, 128, 8, 8]               0\n",
      "          Linear-171             [-1, 8, 8, 16]             256\n",
      "          Linear-172             [-1, 8, 8, 16]             256\n",
      "          Linear-173             [-1, 8, 8, 16]             256\n",
      "          Linear-174            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-175            [-1, 128, 8, 8]               0\n",
      "          Linear-176             [-1, 8, 8, 16]             256\n",
      "          Linear-177             [-1, 8, 8, 16]             256\n",
      "          Linear-178             [-1, 8, 8, 16]             256\n",
      "          Linear-179            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-180            [-1, 128, 8, 8]               0\n",
      "            ReLU-181            [-1, 128, 8, 8]               0\n",
      "          Conv2d-182            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-183            [-1, 256, 8, 8]             512\n",
      "            ReLU-184            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-185            [-1, 256, 8, 8]               0\n",
      "          Conv2d-186            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-187            [-1, 128, 8, 8]             256\n",
      "            ReLU-188            [-1, 128, 8, 8]               0\n",
      "          Linear-189             [-1, 8, 8, 16]             256\n",
      "          Linear-190             [-1, 8, 8, 16]             256\n",
      "          Linear-191             [-1, 8, 8, 16]             256\n",
      "          Linear-192            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-193            [-1, 128, 8, 8]               0\n",
      "          Linear-194             [-1, 8, 8, 16]             256\n",
      "          Linear-195             [-1, 8, 8, 16]             256\n",
      "          Linear-196             [-1, 8, 8, 16]             256\n",
      "          Linear-197            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-198            [-1, 128, 8, 8]               0\n",
      "            ReLU-199            [-1, 128, 8, 8]               0\n",
      "          Conv2d-200            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-201            [-1, 256, 8, 8]             512\n",
      "            ReLU-202            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-203            [-1, 256, 8, 8]               0\n",
      "          Conv2d-204            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-205            [-1, 128, 8, 8]             256\n",
      "            ReLU-206            [-1, 128, 8, 8]               0\n",
      "          Linear-207             [-1, 8, 8, 16]             256\n",
      "          Linear-208             [-1, 8, 8, 16]             256\n",
      "          Linear-209             [-1, 8, 8, 16]             256\n",
      "          Linear-210            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-211            [-1, 128, 8, 8]               0\n",
      "          Linear-212             [-1, 8, 8, 16]             256\n",
      "          Linear-213             [-1, 8, 8, 16]             256\n",
      "          Linear-214             [-1, 8, 8, 16]             256\n",
      "          Linear-215            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-216            [-1, 128, 8, 8]               0\n",
      "            ReLU-217            [-1, 128, 8, 8]               0\n",
      "          Conv2d-218            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-219            [-1, 256, 8, 8]             512\n",
      "            ReLU-220            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-221            [-1, 256, 8, 8]               0\n",
      "         Encoder-222            [-1, 256, 8, 8]               0\n",
      "          Conv2d-223            [-1, 256, 8, 8]          37,888\n",
      "          Conv2d-224            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-225            [-1, 128, 8, 8]             256\n",
      "            ReLU-226            [-1, 128, 8, 8]               0\n",
      "          Linear-227             [-1, 8, 8, 16]             256\n",
      "          Linear-228             [-1, 8, 8, 16]             256\n",
      "          Linear-229             [-1, 8, 8, 16]             256\n",
      "          Linear-230            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-231            [-1, 128, 8, 8]               0\n",
      "          Linear-232             [-1, 8, 8, 16]             256\n",
      "          Linear-233             [-1, 8, 8, 16]             256\n",
      "          Linear-234             [-1, 8, 8, 16]             256\n",
      "          Linear-235            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-236            [-1, 128, 8, 8]               0\n",
      "            ReLU-237            [-1, 128, 8, 8]               0\n",
      "          Conv2d-238            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-239            [-1, 256, 8, 8]             512\n",
      "            ReLU-240            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-241            [-1, 256, 8, 8]               0\n",
      "          Conv2d-242            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-243            [-1, 128, 8, 8]             256\n",
      "            ReLU-244            [-1, 128, 8, 8]               0\n",
      "          Linear-245             [-1, 8, 8, 16]             256\n",
      "          Linear-246             [-1, 8, 8, 16]             256\n",
      "          Linear-247             [-1, 8, 8, 16]             256\n",
      "          Linear-248            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-249            [-1, 128, 8, 8]               0\n",
      "          Linear-250             [-1, 8, 8, 16]             256\n",
      "          Linear-251             [-1, 8, 8, 16]             256\n",
      "          Linear-252             [-1, 8, 8, 16]             256\n",
      "          Linear-253            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-254            [-1, 128, 8, 8]               0\n",
      "            ReLU-255            [-1, 128, 8, 8]               0\n",
      "          Conv2d-256            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-257            [-1, 256, 8, 8]             512\n",
      "            ReLU-258            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-259            [-1, 256, 8, 8]               0\n",
      "          Conv2d-260            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-261            [-1, 128, 8, 8]             256\n",
      "            ReLU-262            [-1, 128, 8, 8]               0\n",
      "          Linear-263             [-1, 8, 8, 16]             256\n",
      "          Linear-264             [-1, 8, 8, 16]             256\n",
      "          Linear-265             [-1, 8, 8, 16]             256\n",
      "          Linear-266            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-267            [-1, 128, 8, 8]               0\n",
      "          Linear-268             [-1, 8, 8, 16]             256\n",
      "          Linear-269             [-1, 8, 8, 16]             256\n",
      "          Linear-270             [-1, 8, 8, 16]             256\n",
      "          Linear-271            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-272            [-1, 128, 8, 8]               0\n",
      "            ReLU-273            [-1, 128, 8, 8]               0\n",
      "          Conv2d-274            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-275            [-1, 256, 8, 8]             512\n",
      "            ReLU-276            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-277            [-1, 256, 8, 8]               0\n",
      "          Conv2d-278            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-279            [-1, 128, 8, 8]             256\n",
      "            ReLU-280            [-1, 128, 8, 8]               0\n",
      "          Linear-281             [-1, 8, 8, 16]             256\n",
      "          Linear-282             [-1, 8, 8, 16]             256\n",
      "          Linear-283             [-1, 8, 8, 16]             256\n",
      "          Linear-284            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-285            [-1, 128, 8, 8]               0\n",
      "          Linear-286             [-1, 8, 8, 16]             256\n",
      "          Linear-287             [-1, 8, 8, 16]             256\n",
      "          Linear-288             [-1, 8, 8, 16]             256\n",
      "          Linear-289            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-290            [-1, 128, 8, 8]               0\n",
      "            ReLU-291            [-1, 128, 8, 8]               0\n",
      "          Conv2d-292            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-293            [-1, 256, 8, 8]             512\n",
      "            ReLU-294            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-295            [-1, 256, 8, 8]               0\n",
      "          Conv2d-296            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-297            [-1, 128, 8, 8]             256\n",
      "            ReLU-298            [-1, 128, 8, 8]               0\n",
      "          Linear-299             [-1, 8, 8, 16]             256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Linear-300             [-1, 8, 8, 16]             256\n",
      "          Linear-301             [-1, 8, 8, 16]             256\n",
      "          Linear-302            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-303            [-1, 128, 8, 8]               0\n",
      "          Linear-304             [-1, 8, 8, 16]             256\n",
      "          Linear-305             [-1, 8, 8, 16]             256\n",
      "          Linear-306             [-1, 8, 8, 16]             256\n",
      "          Linear-307            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-308            [-1, 128, 8, 8]               0\n",
      "            ReLU-309            [-1, 128, 8, 8]               0\n",
      "          Conv2d-310            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-311            [-1, 256, 8, 8]             512\n",
      "            ReLU-312            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-313            [-1, 256, 8, 8]               0\n",
      "         Encoder-314            [-1, 256, 8, 8]               0\n",
      "          Conv2d-315            [-1, 256, 8, 8]          37,888\n",
      "          Conv2d-316            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-317            [-1, 128, 8, 8]             256\n",
      "            ReLU-318            [-1, 128, 8, 8]               0\n",
      "          Linear-319             [-1, 8, 8, 16]             256\n",
      "          Linear-320             [-1, 8, 8, 16]             256\n",
      "          Linear-321             [-1, 8, 8, 16]             256\n",
      "          Linear-322            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-323            [-1, 128, 8, 8]               0\n",
      "          Linear-324             [-1, 8, 8, 16]             256\n",
      "          Linear-325             [-1, 8, 8, 16]             256\n",
      "          Linear-326             [-1, 8, 8, 16]             256\n",
      "          Linear-327            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-328            [-1, 128, 8, 8]               0\n",
      "            ReLU-329            [-1, 128, 8, 8]               0\n",
      "          Conv2d-330            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-331            [-1, 256, 8, 8]             512\n",
      "            ReLU-332            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-333            [-1, 256, 8, 8]               0\n",
      "          Conv2d-334            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-335            [-1, 128, 8, 8]             256\n",
      "            ReLU-336            [-1, 128, 8, 8]               0\n",
      "          Linear-337             [-1, 8, 8, 16]             256\n",
      "          Linear-338             [-1, 8, 8, 16]             256\n",
      "          Linear-339             [-1, 8, 8, 16]             256\n",
      "          Linear-340            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-341            [-1, 128, 8, 8]               0\n",
      "          Linear-342             [-1, 8, 8, 16]             256\n",
      "          Linear-343             [-1, 8, 8, 16]             256\n",
      "          Linear-344             [-1, 8, 8, 16]             256\n",
      "          Linear-345            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-346            [-1, 128, 8, 8]               0\n",
      "            ReLU-347            [-1, 128, 8, 8]               0\n",
      "          Conv2d-348            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-349            [-1, 256, 8, 8]             512\n",
      "            ReLU-350            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-351            [-1, 256, 8, 8]               0\n",
      "          Conv2d-352            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-353            [-1, 128, 8, 8]             256\n",
      "            ReLU-354            [-1, 128, 8, 8]               0\n",
      "          Linear-355             [-1, 8, 8, 16]             256\n",
      "          Linear-356             [-1, 8, 8, 16]             256\n",
      "          Linear-357             [-1, 8, 8, 16]             256\n",
      "          Linear-358            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-359            [-1, 128, 8, 8]               0\n",
      "          Linear-360             [-1, 8, 8, 16]             256\n",
      "          Linear-361             [-1, 8, 8, 16]             256\n",
      "          Linear-362             [-1, 8, 8, 16]             256\n",
      "          Linear-363            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-364            [-1, 128, 8, 8]               0\n",
      "            ReLU-365            [-1, 128, 8, 8]               0\n",
      "          Conv2d-366            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-367            [-1, 256, 8, 8]             512\n",
      "            ReLU-368            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-369            [-1, 256, 8, 8]               0\n",
      "          Conv2d-370            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-371            [-1, 128, 8, 8]             256\n",
      "            ReLU-372            [-1, 128, 8, 8]               0\n",
      "          Linear-373             [-1, 8, 8, 16]             256\n",
      "          Linear-374             [-1, 8, 8, 16]             256\n",
      "          Linear-375             [-1, 8, 8, 16]             256\n",
      "          Linear-376            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-377            [-1, 128, 8, 8]               0\n",
      "          Linear-378             [-1, 8, 8, 16]             256\n",
      "          Linear-379             [-1, 8, 8, 16]             256\n",
      "          Linear-380             [-1, 8, 8, 16]             256\n",
      "          Linear-381            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-382            [-1, 128, 8, 8]               0\n",
      "            ReLU-383            [-1, 128, 8, 8]               0\n",
      "          Conv2d-384            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-385            [-1, 256, 8, 8]             512\n",
      "            ReLU-386            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-387            [-1, 256, 8, 8]               0\n",
      "          Conv2d-388            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-389            [-1, 128, 8, 8]             256\n",
      "            ReLU-390            [-1, 128, 8, 8]               0\n",
      "          Linear-391             [-1, 8, 8, 16]             256\n",
      "          Linear-392             [-1, 8, 8, 16]             256\n",
      "          Linear-393             [-1, 8, 8, 16]             256\n",
      "          Linear-394            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-395            [-1, 128, 8, 8]               0\n",
      "          Linear-396             [-1, 8, 8, 16]             256\n",
      "          Linear-397             [-1, 8, 8, 16]             256\n",
      "          Linear-398             [-1, 8, 8, 16]             256\n",
      "          Linear-399            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-400            [-1, 128, 8, 8]               0\n",
      "            ReLU-401            [-1, 128, 8, 8]               0\n",
      "          Conv2d-402            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-403            [-1, 256, 8, 8]             512\n",
      "            ReLU-404            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-405            [-1, 256, 8, 8]               0\n",
      "         Encoder-406            [-1, 256, 8, 8]               0\n",
      "          Conv2d-407            [-1, 256, 8, 8]          37,888\n",
      "          Conv2d-408            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-409            [-1, 128, 8, 8]             256\n",
      "            ReLU-410            [-1, 128, 8, 8]               0\n",
      "          Linear-411             [-1, 8, 8, 16]             256\n",
      "          Linear-412             [-1, 8, 8, 16]             256\n",
      "          Linear-413             [-1, 8, 8, 16]             256\n",
      "          Linear-414            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-415            [-1, 128, 8, 8]               0\n",
      "          Linear-416             [-1, 8, 8, 16]             256\n",
      "          Linear-417             [-1, 8, 8, 16]             256\n",
      "          Linear-418             [-1, 8, 8, 16]             256\n",
      "          Linear-419            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-420            [-1, 128, 8, 8]               0\n",
      "            ReLU-421            [-1, 128, 8, 8]               0\n",
      "          Conv2d-422            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-423            [-1, 256, 8, 8]             512\n",
      "            ReLU-424            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-425            [-1, 256, 8, 8]               0\n",
      "          Conv2d-426            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-427            [-1, 128, 8, 8]             256\n",
      "            ReLU-428            [-1, 128, 8, 8]               0\n",
      "          Linear-429             [-1, 8, 8, 16]             256\n",
      "          Linear-430             [-1, 8, 8, 16]             256\n",
      "          Linear-431             [-1, 8, 8, 16]             256\n",
      "          Linear-432            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-433            [-1, 128, 8, 8]               0\n",
      "          Linear-434             [-1, 8, 8, 16]             256\n",
      "          Linear-435             [-1, 8, 8, 16]             256\n",
      "          Linear-436             [-1, 8, 8, 16]             256\n",
      "          Linear-437            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-438            [-1, 128, 8, 8]               0\n",
      "            ReLU-439            [-1, 128, 8, 8]               0\n",
      "          Conv2d-440            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-441            [-1, 256, 8, 8]             512\n",
      "            ReLU-442            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-443            [-1, 256, 8, 8]               0\n",
      "          Conv2d-444            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-445            [-1, 128, 8, 8]             256\n",
      "            ReLU-446            [-1, 128, 8, 8]               0\n",
      "          Linear-447             [-1, 8, 8, 16]             256\n",
      "          Linear-448             [-1, 8, 8, 16]             256\n",
      "          Linear-449             [-1, 8, 8, 16]             256\n",
      "          Linear-450            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-451            [-1, 128, 8, 8]               0\n",
      "          Linear-452             [-1, 8, 8, 16]             256\n",
      "          Linear-453             [-1, 8, 8, 16]             256\n",
      "          Linear-454             [-1, 8, 8, 16]             256\n",
      "          Linear-455            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-456            [-1, 128, 8, 8]               0\n",
      "            ReLU-457            [-1, 128, 8, 8]               0\n",
      "          Conv2d-458            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-459            [-1, 256, 8, 8]             512\n",
      "            ReLU-460            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-461            [-1, 256, 8, 8]               0\n",
      "          Conv2d-462            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-463            [-1, 128, 8, 8]             256\n",
      "            ReLU-464            [-1, 128, 8, 8]               0\n",
      "          Linear-465             [-1, 8, 8, 16]             256\n",
      "          Linear-466             [-1, 8, 8, 16]             256\n",
      "          Linear-467             [-1, 8, 8, 16]             256\n",
      "          Linear-468            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-469            [-1, 128, 8, 8]               0\n",
      "          Linear-470             [-1, 8, 8, 16]             256\n",
      "          Linear-471             [-1, 8, 8, 16]             256\n",
      "          Linear-472             [-1, 8, 8, 16]             256\n",
      "          Linear-473            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-474            [-1, 128, 8, 8]               0\n",
      "            ReLU-475            [-1, 128, 8, 8]               0\n",
      "          Conv2d-476            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-477            [-1, 256, 8, 8]             512\n",
      "            ReLU-478            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-479            [-1, 256, 8, 8]               0\n",
      "          Conv2d-480            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-481            [-1, 128, 8, 8]             256\n",
      "            ReLU-482            [-1, 128, 8, 8]               0\n",
      "          Linear-483             [-1, 8, 8, 16]             256\n",
      "          Linear-484             [-1, 8, 8, 16]             256\n",
      "          Linear-485             [-1, 8, 8, 16]             256\n",
      "          Linear-486            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-487            [-1, 128, 8, 8]               0\n",
      "          Linear-488             [-1, 8, 8, 16]             256\n",
      "          Linear-489             [-1, 8, 8, 16]             256\n",
      "          Linear-490             [-1, 8, 8, 16]             256\n",
      "          Linear-491            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-492            [-1, 128, 8, 8]               0\n",
      "            ReLU-493            [-1, 128, 8, 8]               0\n",
      "          Conv2d-494            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-495            [-1, 256, 8, 8]             512\n",
      "            ReLU-496            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-497            [-1, 256, 8, 8]               0\n",
      "         Encoder-498            [-1, 256, 8, 8]               0\n",
      "          Conv2d-499            [-1, 256, 8, 8]          37,888\n",
      "          Conv2d-500            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-501            [-1, 128, 8, 8]             256\n",
      "            ReLU-502            [-1, 128, 8, 8]               0\n",
      "          Linear-503             [-1, 8, 8, 16]             256\n",
      "          Linear-504             [-1, 8, 8, 16]             256\n",
      "          Linear-505             [-1, 8, 8, 16]             256\n",
      "          Linear-506            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-507            [-1, 128, 8, 8]               0\n",
      "          Linear-508             [-1, 8, 8, 16]             256\n",
      "          Linear-509             [-1, 8, 8, 16]             256\n",
      "          Linear-510             [-1, 8, 8, 16]             256\n",
      "          Linear-511            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-512            [-1, 128, 8, 8]               0\n",
      "            ReLU-513            [-1, 128, 8, 8]               0\n",
      "          Conv2d-514            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-515            [-1, 256, 8, 8]             512\n",
      "            ReLU-516            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-517            [-1, 256, 8, 8]               0\n",
      "          Conv2d-518            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-519            [-1, 128, 8, 8]             256\n",
      "            ReLU-520            [-1, 128, 8, 8]               0\n",
      "          Linear-521             [-1, 8, 8, 16]             256\n",
      "          Linear-522             [-1, 8, 8, 16]             256\n",
      "          Linear-523             [-1, 8, 8, 16]             256\n",
      "          Linear-524            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-525            [-1, 128, 8, 8]               0\n",
      "          Linear-526             [-1, 8, 8, 16]             256\n",
      "          Linear-527             [-1, 8, 8, 16]             256\n",
      "          Linear-528             [-1, 8, 8, 16]             256\n",
      "          Linear-529            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-530            [-1, 128, 8, 8]               0\n",
      "            ReLU-531            [-1, 128, 8, 8]               0\n",
      "          Conv2d-532            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-533            [-1, 256, 8, 8]             512\n",
      "            ReLU-534            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-535            [-1, 256, 8, 8]               0\n",
      "          Conv2d-536            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-537            [-1, 128, 8, 8]             256\n",
      "            ReLU-538            [-1, 128, 8, 8]               0\n",
      "          Linear-539             [-1, 8, 8, 16]             256\n",
      "          Linear-540             [-1, 8, 8, 16]             256\n",
      "          Linear-541             [-1, 8, 8, 16]             256\n",
      "          Linear-542            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-543            [-1, 128, 8, 8]               0\n",
      "          Linear-544             [-1, 8, 8, 16]             256\n",
      "          Linear-545             [-1, 8, 8, 16]             256\n",
      "          Linear-546             [-1, 8, 8, 16]             256\n",
      "          Linear-547            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-548            [-1, 128, 8, 8]               0\n",
      "            ReLU-549            [-1, 128, 8, 8]               0\n",
      "          Conv2d-550            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-551            [-1, 256, 8, 8]             512\n",
      "            ReLU-552            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-553            [-1, 256, 8, 8]               0\n",
      "          Conv2d-554            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-555            [-1, 128, 8, 8]             256\n",
      "            ReLU-556            [-1, 128, 8, 8]               0\n",
      "          Linear-557             [-1, 8, 8, 16]             256\n",
      "          Linear-558             [-1, 8, 8, 16]             256\n",
      "          Linear-559             [-1, 8, 8, 16]             256\n",
      "          Linear-560            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-561            [-1, 128, 8, 8]               0\n",
      "          Linear-562             [-1, 8, 8, 16]             256\n",
      "          Linear-563             [-1, 8, 8, 16]             256\n",
      "          Linear-564             [-1, 8, 8, 16]             256\n",
      "          Linear-565            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-566            [-1, 128, 8, 8]               0\n",
      "            ReLU-567            [-1, 128, 8, 8]               0\n",
      "          Conv2d-568            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-569            [-1, 256, 8, 8]             512\n",
      "            ReLU-570            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-571            [-1, 256, 8, 8]               0\n",
      "          Conv2d-572            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-573            [-1, 128, 8, 8]             256\n",
      "            ReLU-574            [-1, 128, 8, 8]               0\n",
      "          Linear-575             [-1, 8, 8, 16]             256\n",
      "          Linear-576             [-1, 8, 8, 16]             256\n",
      "          Linear-577             [-1, 8, 8, 16]             256\n",
      "          Linear-578            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-579            [-1, 128, 8, 8]               0\n",
      "          Linear-580             [-1, 8, 8, 16]             256\n",
      "          Linear-581             [-1, 8, 8, 16]             256\n",
      "          Linear-582             [-1, 8, 8, 16]             256\n",
      "          Linear-583            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-584            [-1, 128, 8, 8]               0\n",
      "            ReLU-585            [-1, 128, 8, 8]               0\n",
      "          Conv2d-586            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-587            [-1, 256, 8, 8]             512\n",
      "            ReLU-588            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-589            [-1, 256, 8, 8]               0\n",
      "         Encoder-590            [-1, 256, 8, 8]               0\n",
      "          Conv2d-591            [-1, 256, 8, 8]          37,888\n",
      "          Conv2d-592            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-593            [-1, 128, 8, 8]             256\n",
      "            ReLU-594            [-1, 128, 8, 8]               0\n",
      "          Linear-595             [-1, 8, 8, 16]             256\n",
      "          Linear-596             [-1, 8, 8, 16]             256\n",
      "          Linear-597             [-1, 8, 8, 16]             256\n",
      "          Linear-598            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-599            [-1, 128, 8, 8]               0\n",
      "          Linear-600             [-1, 8, 8, 16]             256\n",
      "          Linear-601             [-1, 8, 8, 16]             256\n",
      "          Linear-602             [-1, 8, 8, 16]             256\n",
      "          Linear-603            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-604            [-1, 128, 8, 8]               0\n",
      "            ReLU-605            [-1, 128, 8, 8]               0\n",
      "          Conv2d-606            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-607            [-1, 256, 8, 8]             512\n",
      "            ReLU-608            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-609            [-1, 256, 8, 8]               0\n",
      "          Conv2d-610            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-611            [-1, 128, 8, 8]             256\n",
      "            ReLU-612            [-1, 128, 8, 8]               0\n",
      "          Linear-613             [-1, 8, 8, 16]             256\n",
      "          Linear-614             [-1, 8, 8, 16]             256\n",
      "          Linear-615             [-1, 8, 8, 16]             256\n",
      "          Linear-616            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-617            [-1, 128, 8, 8]               0\n",
      "          Linear-618             [-1, 8, 8, 16]             256\n",
      "          Linear-619             [-1, 8, 8, 16]             256\n",
      "          Linear-620             [-1, 8, 8, 16]             256\n",
      "          Linear-621            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-622            [-1, 128, 8, 8]               0\n",
      "            ReLU-623            [-1, 128, 8, 8]               0\n",
      "          Conv2d-624            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-625            [-1, 256, 8, 8]             512\n",
      "            ReLU-626            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-627            [-1, 256, 8, 8]               0\n",
      "          Conv2d-628            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-629            [-1, 128, 8, 8]             256\n",
      "            ReLU-630            [-1, 128, 8, 8]               0\n",
      "          Linear-631             [-1, 8, 8, 16]             256\n",
      "          Linear-632             [-1, 8, 8, 16]             256\n",
      "          Linear-633             [-1, 8, 8, 16]             256\n",
      "          Linear-634            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-635            [-1, 128, 8, 8]               0\n",
      "          Linear-636             [-1, 8, 8, 16]             256\n",
      "          Linear-637             [-1, 8, 8, 16]             256\n",
      "          Linear-638             [-1, 8, 8, 16]             256\n",
      "          Linear-639            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-640            [-1, 128, 8, 8]               0\n",
      "            ReLU-641            [-1, 128, 8, 8]               0\n",
      "          Conv2d-642            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-643            [-1, 256, 8, 8]             512\n",
      "            ReLU-644            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-645            [-1, 256, 8, 8]               0\n",
      "          Conv2d-646            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-647            [-1, 128, 8, 8]             256\n",
      "            ReLU-648            [-1, 128, 8, 8]               0\n",
      "          Linear-649             [-1, 8, 8, 16]             256\n",
      "          Linear-650             [-1, 8, 8, 16]             256\n",
      "          Linear-651             [-1, 8, 8, 16]             256\n",
      "          Linear-652            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-653            [-1, 128, 8, 8]               0\n",
      "          Linear-654             [-1, 8, 8, 16]             256\n",
      "          Linear-655             [-1, 8, 8, 16]             256\n",
      "          Linear-656             [-1, 8, 8, 16]             256\n",
      "          Linear-657            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-658            [-1, 128, 8, 8]               0\n",
      "            ReLU-659            [-1, 128, 8, 8]               0\n",
      "          Conv2d-660            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-661            [-1, 256, 8, 8]             512\n",
      "            ReLU-662            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-663            [-1, 256, 8, 8]               0\n",
      "          Conv2d-664            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-665            [-1, 128, 8, 8]             256\n",
      "            ReLU-666            [-1, 128, 8, 8]               0\n",
      "          Linear-667             [-1, 8, 8, 16]             256\n",
      "          Linear-668             [-1, 8, 8, 16]             256\n",
      "          Linear-669             [-1, 8, 8, 16]             256\n",
      "          Linear-670            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-671            [-1, 128, 8, 8]               0\n",
      "          Linear-672             [-1, 8, 8, 16]             256\n",
      "          Linear-673             [-1, 8, 8, 16]             256\n",
      "          Linear-674             [-1, 8, 8, 16]             256\n",
      "          Linear-675            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-676            [-1, 128, 8, 8]               0\n",
      "            ReLU-677            [-1, 128, 8, 8]               0\n",
      "          Conv2d-678            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-679            [-1, 256, 8, 8]             512\n",
      "            ReLU-680            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-681            [-1, 256, 8, 8]               0\n",
      "         Encoder-682            [-1, 256, 8, 8]               0\n",
      "          Conv2d-683            [-1, 256, 8, 8]          37,888\n",
      "          Conv2d-684            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-685            [-1, 128, 8, 8]             256\n",
      "            ReLU-686            [-1, 128, 8, 8]               0\n",
      "          Linear-687             [-1, 8, 8, 16]             256\n",
      "          Linear-688             [-1, 8, 8, 16]             256\n",
      "          Linear-689             [-1, 8, 8, 16]             256\n",
      "          Linear-690            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-691            [-1, 128, 8, 8]               0\n",
      "          Linear-692             [-1, 8, 8, 16]             256\n",
      "          Linear-693             [-1, 8, 8, 16]             256\n",
      "          Linear-694             [-1, 8, 8, 16]             256\n",
      "          Linear-695            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-696            [-1, 128, 8, 8]               0\n",
      "            ReLU-697            [-1, 128, 8, 8]               0\n",
      "          Conv2d-698            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-699            [-1, 256, 8, 8]             512\n",
      "            ReLU-700            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-701            [-1, 256, 8, 8]               0\n",
      "          Conv2d-702            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-703            [-1, 128, 8, 8]             256\n",
      "            ReLU-704            [-1, 128, 8, 8]               0\n",
      "          Linear-705             [-1, 8, 8, 16]             256\n",
      "          Linear-706             [-1, 8, 8, 16]             256\n",
      "          Linear-707             [-1, 8, 8, 16]             256\n",
      "          Linear-708            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-709            [-1, 128, 8, 8]               0\n",
      "          Linear-710             [-1, 8, 8, 16]             256\n",
      "          Linear-711             [-1, 8, 8, 16]             256\n",
      "          Linear-712             [-1, 8, 8, 16]             256\n",
      "          Linear-713            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-714            [-1, 128, 8, 8]               0\n",
      "            ReLU-715            [-1, 128, 8, 8]               0\n",
      "          Conv2d-716            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-717            [-1, 256, 8, 8]             512\n",
      "            ReLU-718            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-719            [-1, 256, 8, 8]               0\n",
      "          Conv2d-720            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-721            [-1, 128, 8, 8]             256\n",
      "            ReLU-722            [-1, 128, 8, 8]               0\n",
      "          Linear-723             [-1, 8, 8, 16]             256\n",
      "          Linear-724             [-1, 8, 8, 16]             256\n",
      "          Linear-725             [-1, 8, 8, 16]             256\n",
      "          Linear-726            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-727            [-1, 128, 8, 8]               0\n",
      "          Linear-728             [-1, 8, 8, 16]             256\n",
      "          Linear-729             [-1, 8, 8, 16]             256\n",
      "          Linear-730             [-1, 8, 8, 16]             256\n",
      "          Linear-731            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-732            [-1, 128, 8, 8]               0\n",
      "            ReLU-733            [-1, 128, 8, 8]               0\n",
      "          Conv2d-734            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-735            [-1, 256, 8, 8]             512\n",
      "            ReLU-736            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-737            [-1, 256, 8, 8]               0\n",
      "          Conv2d-738            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-739            [-1, 128, 8, 8]             256\n",
      "            ReLU-740            [-1, 128, 8, 8]               0\n",
      "          Linear-741             [-1, 8, 8, 16]             256\n",
      "          Linear-742             [-1, 8, 8, 16]             256\n",
      "          Linear-743             [-1, 8, 8, 16]             256\n",
      "          Linear-744            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-745            [-1, 128, 8, 8]               0\n",
      "          Linear-746             [-1, 8, 8, 16]             256\n",
      "          Linear-747             [-1, 8, 8, 16]             256\n",
      "          Linear-748             [-1, 8, 8, 16]             256\n",
      "          Linear-749            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-750            [-1, 128, 8, 8]               0\n",
      "            ReLU-751            [-1, 128, 8, 8]               0\n",
      "          Conv2d-752            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-753            [-1, 256, 8, 8]             512\n",
      "            ReLU-754            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-755            [-1, 256, 8, 8]               0\n",
      "          Conv2d-756            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-757            [-1, 128, 8, 8]             256\n",
      "            ReLU-758            [-1, 128, 8, 8]               0\n",
      "          Linear-759             [-1, 8, 8, 16]             256\n",
      "          Linear-760             [-1, 8, 8, 16]             256\n",
      "          Linear-761             [-1, 8, 8, 16]             256\n",
      "          Linear-762            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-763            [-1, 128, 8, 8]               0\n",
      "          Linear-764             [-1, 8, 8, 16]             256\n",
      "          Linear-765             [-1, 8, 8, 16]             256\n",
      "          Linear-766             [-1, 8, 8, 16]             256\n",
      "          Linear-767            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-768            [-1, 128, 8, 8]               0\n",
      "            ReLU-769            [-1, 128, 8, 8]               0\n",
      "          Conv2d-770            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-771            [-1, 256, 8, 8]             512\n",
      "            ReLU-772            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-773            [-1, 256, 8, 8]               0\n",
      "         Encoder-774            [-1, 256, 8, 8]               0\n",
      "          Conv2d-775            [-1, 256, 8, 8]          37,888\n",
      "          Conv2d-776            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-777            [-1, 128, 8, 8]             256\n",
      "            ReLU-778            [-1, 128, 8, 8]               0\n",
      "          Linear-779             [-1, 8, 8, 16]             256\n",
      "          Linear-780             [-1, 8, 8, 16]             256\n",
      "          Linear-781             [-1, 8, 8, 16]             256\n",
      "          Linear-782            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-783            [-1, 128, 8, 8]               0\n",
      "          Linear-784             [-1, 8, 8, 16]             256\n",
      "          Linear-785             [-1, 8, 8, 16]             256\n",
      "          Linear-786             [-1, 8, 8, 16]             256\n",
      "          Linear-787            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-788            [-1, 128, 8, 8]               0\n",
      "            ReLU-789            [-1, 128, 8, 8]               0\n",
      "          Conv2d-790            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-791            [-1, 256, 8, 8]             512\n",
      "            ReLU-792            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-793            [-1, 256, 8, 8]               0\n",
      "          Conv2d-794            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-795            [-1, 128, 8, 8]             256\n",
      "            ReLU-796            [-1, 128, 8, 8]               0\n",
      "          Linear-797             [-1, 8, 8, 16]             256\n",
      "          Linear-798             [-1, 8, 8, 16]             256\n",
      "          Linear-799             [-1, 8, 8, 16]             256\n",
      "          Linear-800            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-801            [-1, 128, 8, 8]               0\n",
      "          Linear-802             [-1, 8, 8, 16]             256\n",
      "          Linear-803             [-1, 8, 8, 16]             256\n",
      "          Linear-804             [-1, 8, 8, 16]             256\n",
      "          Linear-805            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-806            [-1, 128, 8, 8]               0\n",
      "            ReLU-807            [-1, 128, 8, 8]               0\n",
      "          Conv2d-808            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-809            [-1, 256, 8, 8]             512\n",
      "            ReLU-810            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-811            [-1, 256, 8, 8]               0\n",
      "          Conv2d-812            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-813            [-1, 128, 8, 8]             256\n",
      "            ReLU-814            [-1, 128, 8, 8]               0\n",
      "          Linear-815             [-1, 8, 8, 16]             256\n",
      "          Linear-816             [-1, 8, 8, 16]             256\n",
      "          Linear-817             [-1, 8, 8, 16]             256\n",
      "          Linear-818            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-819            [-1, 128, 8, 8]               0\n",
      "          Linear-820             [-1, 8, 8, 16]             256\n",
      "          Linear-821             [-1, 8, 8, 16]             256\n",
      "          Linear-822             [-1, 8, 8, 16]             256\n",
      "          Linear-823            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-824            [-1, 128, 8, 8]               0\n",
      "            ReLU-825            [-1, 128, 8, 8]               0\n",
      "          Conv2d-826            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-827            [-1, 256, 8, 8]             512\n",
      "            ReLU-828            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-829            [-1, 256, 8, 8]               0\n",
      "          Conv2d-830            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-831            [-1, 128, 8, 8]             256\n",
      "            ReLU-832            [-1, 128, 8, 8]               0\n",
      "          Linear-833             [-1, 8, 8, 16]             256\n",
      "          Linear-834             [-1, 8, 8, 16]             256\n",
      "          Linear-835             [-1, 8, 8, 16]             256\n",
      "          Linear-836            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-837            [-1, 128, 8, 8]               0\n",
      "          Linear-838             [-1, 8, 8, 16]             256\n",
      "          Linear-839             [-1, 8, 8, 16]             256\n",
      "          Linear-840             [-1, 8, 8, 16]             256\n",
      "          Linear-841            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-842            [-1, 128, 8, 8]               0\n",
      "            ReLU-843            [-1, 128, 8, 8]               0\n",
      "          Conv2d-844            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-845            [-1, 256, 8, 8]             512\n",
      "            ReLU-846            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-847            [-1, 256, 8, 8]               0\n",
      "          Conv2d-848            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-849            [-1, 128, 8, 8]             256\n",
      "            ReLU-850            [-1, 128, 8, 8]               0\n",
      "          Linear-851             [-1, 8, 8, 16]             256\n",
      "          Linear-852             [-1, 8, 8, 16]             256\n",
      "          Linear-853             [-1, 8, 8, 16]             256\n",
      "          Linear-854            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-855            [-1, 128, 8, 8]               0\n",
      "          Linear-856             [-1, 8, 8, 16]             256\n",
      "          Linear-857             [-1, 8, 8, 16]             256\n",
      "          Linear-858             [-1, 8, 8, 16]             256\n",
      "          Linear-859            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-860            [-1, 128, 8, 8]               0\n",
      "            ReLU-861            [-1, 128, 8, 8]               0\n",
      "          Conv2d-862            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-863            [-1, 256, 8, 8]             512\n",
      "            ReLU-864            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-865            [-1, 256, 8, 8]               0\n",
      "         Encoder-866            [-1, 256, 8, 8]               0\n",
      "          Conv2d-867            [-1, 256, 8, 8]          37,888\n",
      "          Conv2d-868            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-869            [-1, 128, 8, 8]             256\n",
      "            ReLU-870            [-1, 128, 8, 8]               0\n",
      "          Linear-871             [-1, 8, 8, 16]             256\n",
      "          Linear-872             [-1, 8, 8, 16]             256\n",
      "          Linear-873             [-1, 8, 8, 16]             256\n",
      "          Linear-874            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-875            [-1, 128, 8, 8]               0\n",
      "          Linear-876             [-1, 8, 8, 16]             256\n",
      "          Linear-877             [-1, 8, 8, 16]             256\n",
      "          Linear-878             [-1, 8, 8, 16]             256\n",
      "          Linear-879            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-880            [-1, 128, 8, 8]               0\n",
      "            ReLU-881            [-1, 128, 8, 8]               0\n",
      "          Conv2d-882            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-883            [-1, 256, 8, 8]             512\n",
      "            ReLU-884            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-885            [-1, 256, 8, 8]               0\n",
      "          Conv2d-886            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-887            [-1, 128, 8, 8]             256\n",
      "            ReLU-888            [-1, 128, 8, 8]               0\n",
      "          Linear-889             [-1, 8, 8, 16]             256\n",
      "          Linear-890             [-1, 8, 8, 16]             256\n",
      "          Linear-891             [-1, 8, 8, 16]             256\n",
      "          Linear-892            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-893            [-1, 128, 8, 8]               0\n",
      "          Linear-894             [-1, 8, 8, 16]             256\n",
      "          Linear-895             [-1, 8, 8, 16]             256\n",
      "          Linear-896             [-1, 8, 8, 16]             256\n",
      "          Linear-897            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-898            [-1, 128, 8, 8]               0\n",
      "            ReLU-899            [-1, 128, 8, 8]               0\n",
      "          Conv2d-900            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-901            [-1, 256, 8, 8]             512\n",
      "            ReLU-902            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-903            [-1, 256, 8, 8]               0\n",
      "          Conv2d-904            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-905            [-1, 128, 8, 8]             256\n",
      "            ReLU-906            [-1, 128, 8, 8]               0\n",
      "          Linear-907             [-1, 8, 8, 16]             256\n",
      "          Linear-908             [-1, 8, 8, 16]             256\n",
      "          Linear-909             [-1, 8, 8, 16]             256\n",
      "          Linear-910            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-911            [-1, 128, 8, 8]               0\n",
      "          Linear-912             [-1, 8, 8, 16]             256\n",
      "          Linear-913             [-1, 8, 8, 16]             256\n",
      "          Linear-914             [-1, 8, 8, 16]             256\n",
      "          Linear-915            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-916            [-1, 128, 8, 8]               0\n",
      "            ReLU-917            [-1, 128, 8, 8]               0\n",
      "          Conv2d-918            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-919            [-1, 256, 8, 8]             512\n",
      "            ReLU-920            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-921            [-1, 256, 8, 8]               0\n",
      "          Conv2d-922            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-923            [-1, 128, 8, 8]             256\n",
      "            ReLU-924            [-1, 128, 8, 8]               0\n",
      "          Linear-925             [-1, 8, 8, 16]             256\n",
      "          Linear-926             [-1, 8, 8, 16]             256\n",
      "          Linear-927             [-1, 8, 8, 16]             256\n",
      "          Linear-928            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-929            [-1, 128, 8, 8]               0\n",
      "          Linear-930             [-1, 8, 8, 16]             256\n",
      "          Linear-931             [-1, 8, 8, 16]             256\n",
      "          Linear-932             [-1, 8, 8, 16]             256\n",
      "          Linear-933            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-934            [-1, 128, 8, 8]               0\n",
      "            ReLU-935            [-1, 128, 8, 8]               0\n",
      "          Conv2d-936            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-937            [-1, 256, 8, 8]             512\n",
      "            ReLU-938            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-939            [-1, 256, 8, 8]               0\n",
      "          Conv2d-940            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-941            [-1, 128, 8, 8]             256\n",
      "            ReLU-942            [-1, 128, 8, 8]               0\n",
      "          Linear-943             [-1, 8, 8, 16]             256\n",
      "          Linear-944             [-1, 8, 8, 16]             256\n",
      "          Linear-945             [-1, 8, 8, 16]             256\n",
      "          Linear-946            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-947            [-1, 128, 8, 8]               0\n",
      "          Linear-948             [-1, 8, 8, 16]             256\n",
      "          Linear-949             [-1, 8, 8, 16]             256\n",
      "          Linear-950             [-1, 8, 8, 16]             256\n",
      "          Linear-951            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-952            [-1, 128, 8, 8]               0\n",
      "            ReLU-953            [-1, 128, 8, 8]               0\n",
      "          Conv2d-954            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-955            [-1, 256, 8, 8]             512\n",
      "            ReLU-956            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-957            [-1, 256, 8, 8]               0\n",
      "         Encoder-958            [-1, 256, 8, 8]               0\n",
      "          Conv2d-959            [-1, 256, 8, 8]          37,888\n",
      "          Conv2d-960            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-961            [-1, 128, 8, 8]             256\n",
      "            ReLU-962            [-1, 128, 8, 8]               0\n",
      "          Linear-963             [-1, 8, 8, 16]             256\n",
      "          Linear-964             [-1, 8, 8, 16]             256\n",
      "          Linear-965             [-1, 8, 8, 16]             256\n",
      "          Linear-966            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-967            [-1, 128, 8, 8]               0\n",
      "          Linear-968             [-1, 8, 8, 16]             256\n",
      "          Linear-969             [-1, 8, 8, 16]             256\n",
      "          Linear-970             [-1, 8, 8, 16]             256\n",
      "          Linear-971            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-972            [-1, 128, 8, 8]               0\n",
      "            ReLU-973            [-1, 128, 8, 8]               0\n",
      "          Conv2d-974            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-975            [-1, 256, 8, 8]             512\n",
      "            ReLU-976            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-977            [-1, 256, 8, 8]               0\n",
      "          Conv2d-978            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-979            [-1, 128, 8, 8]             256\n",
      "            ReLU-980            [-1, 128, 8, 8]               0\n",
      "          Linear-981             [-1, 8, 8, 16]             256\n",
      "          Linear-982             [-1, 8, 8, 16]             256\n",
      "          Linear-983             [-1, 8, 8, 16]             256\n",
      "          Linear-984            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-985            [-1, 128, 8, 8]               0\n",
      "          Linear-986             [-1, 8, 8, 16]             256\n",
      "          Linear-987             [-1, 8, 8, 16]             256\n",
      "          Linear-988             [-1, 8, 8, 16]             256\n",
      "          Linear-989            [-1, 8, 8, 128]          16,512\n",
      "  AxialAttention-990            [-1, 128, 8, 8]               0\n",
      "            ReLU-991            [-1, 128, 8, 8]               0\n",
      "          Conv2d-992            [-1, 256, 8, 8]          33,024\n",
      "     BatchNorm2d-993            [-1, 256, 8, 8]             512\n",
      "            ReLU-994            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-995            [-1, 256, 8, 8]               0\n",
      "          Conv2d-996            [-1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-997            [-1, 128, 8, 8]             256\n",
      "            ReLU-998            [-1, 128, 8, 8]               0\n",
      "          Linear-999             [-1, 8, 8, 16]             256\n",
      "         Linear-1000             [-1, 8, 8, 16]             256\n",
      "         Linear-1001             [-1, 8, 8, 16]             256\n",
      "         Linear-1002            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1003            [-1, 128, 8, 8]               0\n",
      "         Linear-1004             [-1, 8, 8, 16]             256\n",
      "         Linear-1005             [-1, 8, 8, 16]             256\n",
      "         Linear-1006             [-1, 8, 8, 16]             256\n",
      "         Linear-1007            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1008            [-1, 128, 8, 8]               0\n",
      "           ReLU-1009            [-1, 128, 8, 8]               0\n",
      "         Conv2d-1010            [-1, 256, 8, 8]          33,024\n",
      "    BatchNorm2d-1011            [-1, 256, 8, 8]             512\n",
      "           ReLU-1012            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-1013            [-1, 256, 8, 8]               0\n",
      "         Conv2d-1014            [-1, 128, 8, 8]          32,896\n",
      "    BatchNorm2d-1015            [-1, 128, 8, 8]             256\n",
      "           ReLU-1016            [-1, 128, 8, 8]               0\n",
      "         Linear-1017             [-1, 8, 8, 16]             256\n",
      "         Linear-1018             [-1, 8, 8, 16]             256\n",
      "         Linear-1019             [-1, 8, 8, 16]             256\n",
      "         Linear-1020            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1021            [-1, 128, 8, 8]               0\n",
      "         Linear-1022             [-1, 8, 8, 16]             256\n",
      "         Linear-1023             [-1, 8, 8, 16]             256\n",
      "         Linear-1024             [-1, 8, 8, 16]             256\n",
      "         Linear-1025            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1026            [-1, 128, 8, 8]               0\n",
      "           ReLU-1027            [-1, 128, 8, 8]               0\n",
      "         Conv2d-1028            [-1, 256, 8, 8]          33,024\n",
      "    BatchNorm2d-1029            [-1, 256, 8, 8]             512\n",
      "           ReLU-1030            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-1031            [-1, 256, 8, 8]               0\n",
      "         Conv2d-1032            [-1, 128, 8, 8]          32,896\n",
      "    BatchNorm2d-1033            [-1, 128, 8, 8]             256\n",
      "           ReLU-1034            [-1, 128, 8, 8]               0\n",
      "         Linear-1035             [-1, 8, 8, 16]             256\n",
      "         Linear-1036             [-1, 8, 8, 16]             256\n",
      "         Linear-1037             [-1, 8, 8, 16]             256\n",
      "         Linear-1038            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1039            [-1, 128, 8, 8]               0\n",
      "         Linear-1040             [-1, 8, 8, 16]             256\n",
      "         Linear-1041             [-1, 8, 8, 16]             256\n",
      "         Linear-1042             [-1, 8, 8, 16]             256\n",
      "         Linear-1043            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1044            [-1, 128, 8, 8]               0\n",
      "           ReLU-1045            [-1, 128, 8, 8]               0\n",
      "         Conv2d-1046            [-1, 256, 8, 8]          33,024\n",
      "    BatchNorm2d-1047            [-1, 256, 8, 8]             512\n",
      "           ReLU-1048            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-1049            [-1, 256, 8, 8]               0\n",
      "        Encoder-1050            [-1, 256, 8, 8]               0\n",
      "         Conv2d-1051            [-1, 256, 8, 8]          37,888\n",
      "         Conv2d-1052            [-1, 128, 8, 8]          32,896\n",
      "    BatchNorm2d-1053            [-1, 128, 8, 8]             256\n",
      "           ReLU-1054            [-1, 128, 8, 8]               0\n",
      "         Linear-1055             [-1, 8, 8, 16]             256\n",
      "         Linear-1056             [-1, 8, 8, 16]             256\n",
      "         Linear-1057             [-1, 8, 8, 16]             256\n",
      "         Linear-1058            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1059            [-1, 128, 8, 8]               0\n",
      "         Linear-1060             [-1, 8, 8, 16]             256\n",
      "         Linear-1061             [-1, 8, 8, 16]             256\n",
      "         Linear-1062             [-1, 8, 8, 16]             256\n",
      "         Linear-1063            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1064            [-1, 128, 8, 8]               0\n",
      "           ReLU-1065            [-1, 128, 8, 8]               0\n",
      "         Conv2d-1066            [-1, 256, 8, 8]          33,024\n",
      "    BatchNorm2d-1067            [-1, 256, 8, 8]             512\n",
      "           ReLU-1068            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-1069            [-1, 256, 8, 8]               0\n",
      "         Conv2d-1070            [-1, 128, 8, 8]          32,896\n",
      "    BatchNorm2d-1071            [-1, 128, 8, 8]             256\n",
      "           ReLU-1072            [-1, 128, 8, 8]               0\n",
      "         Linear-1073             [-1, 8, 8, 16]             256\n",
      "         Linear-1074             [-1, 8, 8, 16]             256\n",
      "         Linear-1075             [-1, 8, 8, 16]             256\n",
      "         Linear-1076            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1077            [-1, 128, 8, 8]               0\n",
      "         Linear-1078             [-1, 8, 8, 16]             256\n",
      "         Linear-1079             [-1, 8, 8, 16]             256\n",
      "         Linear-1080             [-1, 8, 8, 16]             256\n",
      "         Linear-1081            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1082            [-1, 128, 8, 8]               0\n",
      "           ReLU-1083            [-1, 128, 8, 8]               0\n",
      "         Conv2d-1084            [-1, 256, 8, 8]          33,024\n",
      "    BatchNorm2d-1085            [-1, 256, 8, 8]             512\n",
      "           ReLU-1086            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-1087            [-1, 256, 8, 8]               0\n",
      "         Conv2d-1088            [-1, 128, 8, 8]          32,896\n",
      "    BatchNorm2d-1089            [-1, 128, 8, 8]             256\n",
      "           ReLU-1090            [-1, 128, 8, 8]               0\n",
      "         Linear-1091             [-1, 8, 8, 16]             256\n",
      "         Linear-1092             [-1, 8, 8, 16]             256\n",
      "         Linear-1093             [-1, 8, 8, 16]             256\n",
      "         Linear-1094            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1095            [-1, 128, 8, 8]               0\n",
      "         Linear-1096             [-1, 8, 8, 16]             256\n",
      "         Linear-1097             [-1, 8, 8, 16]             256\n",
      "         Linear-1098             [-1, 8, 8, 16]             256\n",
      "         Linear-1099            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1100            [-1, 128, 8, 8]               0\n",
      "           ReLU-1101            [-1, 128, 8, 8]               0\n",
      "         Conv2d-1102            [-1, 256, 8, 8]          33,024\n",
      "    BatchNorm2d-1103            [-1, 256, 8, 8]             512\n",
      "           ReLU-1104            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-1105            [-1, 256, 8, 8]               0\n",
      "         Conv2d-1106            [-1, 128, 8, 8]          32,896\n",
      "    BatchNorm2d-1107            [-1, 128, 8, 8]             256\n",
      "           ReLU-1108            [-1, 128, 8, 8]               0\n",
      "         Linear-1109             [-1, 8, 8, 16]             256\n",
      "         Linear-1110             [-1, 8, 8, 16]             256\n",
      "         Linear-1111             [-1, 8, 8, 16]             256\n",
      "         Linear-1112            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1113            [-1, 128, 8, 8]               0\n",
      "         Linear-1114             [-1, 8, 8, 16]             256\n",
      "         Linear-1115             [-1, 8, 8, 16]             256\n",
      "         Linear-1116             [-1, 8, 8, 16]             256\n",
      "         Linear-1117            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1118            [-1, 128, 8, 8]               0\n",
      "           ReLU-1119            [-1, 128, 8, 8]               0\n",
      "         Conv2d-1120            [-1, 256, 8, 8]          33,024\n",
      "    BatchNorm2d-1121            [-1, 256, 8, 8]             512\n",
      "           ReLU-1122            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-1123            [-1, 256, 8, 8]               0\n",
      "         Conv2d-1124            [-1, 128, 8, 8]          32,896\n",
      "    BatchNorm2d-1125            [-1, 128, 8, 8]             256\n",
      "           ReLU-1126            [-1, 128, 8, 8]               0\n",
      "         Linear-1127             [-1, 8, 8, 16]             256\n",
      "         Linear-1128             [-1, 8, 8, 16]             256\n",
      "         Linear-1129             [-1, 8, 8, 16]             256\n",
      "         Linear-1130            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1131            [-1, 128, 8, 8]               0\n",
      "         Linear-1132             [-1, 8, 8, 16]             256\n",
      "         Linear-1133             [-1, 8, 8, 16]             256\n",
      "         Linear-1134             [-1, 8, 8, 16]             256\n",
      "         Linear-1135            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1136            [-1, 128, 8, 8]               0\n",
      "           ReLU-1137            [-1, 128, 8, 8]               0\n",
      "         Conv2d-1138            [-1, 256, 8, 8]          33,024\n",
      "    BatchNorm2d-1139            [-1, 256, 8, 8]             512\n",
      "           ReLU-1140            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-1141            [-1, 256, 8, 8]               0\n",
      "        Encoder-1142            [-1, 256, 8, 8]               0\n",
      "         Conv2d-1143            [-1, 256, 8, 8]          37,888\n",
      "         Conv2d-1144            [-1, 128, 8, 8]          32,896\n",
      "    BatchNorm2d-1145            [-1, 128, 8, 8]             256\n",
      "           ReLU-1146            [-1, 128, 8, 8]               0\n",
      "         Linear-1147             [-1, 8, 8, 16]             256\n",
      "         Linear-1148             [-1, 8, 8, 16]             256\n",
      "         Linear-1149             [-1, 8, 8, 16]             256\n",
      "         Linear-1150            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1151            [-1, 128, 8, 8]               0\n",
      "         Linear-1152             [-1, 8, 8, 16]             256\n",
      "         Linear-1153             [-1, 8, 8, 16]             256\n",
      "         Linear-1154             [-1, 8, 8, 16]             256\n",
      "         Linear-1155            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1156            [-1, 128, 8, 8]               0\n",
      "           ReLU-1157            [-1, 128, 8, 8]               0\n",
      "         Conv2d-1158            [-1, 256, 8, 8]          33,024\n",
      "    BatchNorm2d-1159            [-1, 256, 8, 8]             512\n",
      "           ReLU-1160            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-1161            [-1, 256, 8, 8]               0\n",
      "         Conv2d-1162            [-1, 128, 8, 8]          32,896\n",
      "    BatchNorm2d-1163            [-1, 128, 8, 8]             256\n",
      "           ReLU-1164            [-1, 128, 8, 8]               0\n",
      "         Linear-1165             [-1, 8, 8, 16]             256\n",
      "         Linear-1166             [-1, 8, 8, 16]             256\n",
      "         Linear-1167             [-1, 8, 8, 16]             256\n",
      "         Linear-1168            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1169            [-1, 128, 8, 8]               0\n",
      "         Linear-1170             [-1, 8, 8, 16]             256\n",
      "         Linear-1171             [-1, 8, 8, 16]             256\n",
      "         Linear-1172             [-1, 8, 8, 16]             256\n",
      "         Linear-1173            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1174            [-1, 128, 8, 8]               0\n",
      "           ReLU-1175            [-1, 128, 8, 8]               0\n",
      "         Conv2d-1176            [-1, 256, 8, 8]          33,024\n",
      "    BatchNorm2d-1177            [-1, 256, 8, 8]             512\n",
      "           ReLU-1178            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-1179            [-1, 256, 8, 8]               0\n",
      "         Conv2d-1180            [-1, 128, 8, 8]          32,896\n",
      "    BatchNorm2d-1181            [-1, 128, 8, 8]             256\n",
      "           ReLU-1182            [-1, 128, 8, 8]               0\n",
      "         Linear-1183             [-1, 8, 8, 16]             256\n",
      "         Linear-1184             [-1, 8, 8, 16]             256\n",
      "         Linear-1185             [-1, 8, 8, 16]             256\n",
      "         Linear-1186            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1187            [-1, 128, 8, 8]               0\n",
      "         Linear-1188             [-1, 8, 8, 16]             256\n",
      "         Linear-1189             [-1, 8, 8, 16]             256\n",
      "         Linear-1190             [-1, 8, 8, 16]             256\n",
      "         Linear-1191            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1192            [-1, 128, 8, 8]               0\n",
      "           ReLU-1193            [-1, 128, 8, 8]               0\n",
      "         Conv2d-1194            [-1, 256, 8, 8]          33,024\n",
      "    BatchNorm2d-1195            [-1, 256, 8, 8]             512\n",
      "           ReLU-1196            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-1197            [-1, 256, 8, 8]               0\n",
      "         Conv2d-1198            [-1, 128, 8, 8]          32,896\n",
      "    BatchNorm2d-1199            [-1, 128, 8, 8]             256\n",
      "           ReLU-1200            [-1, 128, 8, 8]               0\n",
      "         Linear-1201             [-1, 8, 8, 16]             256\n",
      "         Linear-1202             [-1, 8, 8, 16]             256\n",
      "         Linear-1203             [-1, 8, 8, 16]             256\n",
      "         Linear-1204            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1205            [-1, 128, 8, 8]               0\n",
      "         Linear-1206             [-1, 8, 8, 16]             256\n",
      "         Linear-1207             [-1, 8, 8, 16]             256\n",
      "         Linear-1208             [-1, 8, 8, 16]             256\n",
      "         Linear-1209            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1210            [-1, 128, 8, 8]               0\n",
      "           ReLU-1211            [-1, 128, 8, 8]               0\n",
      "         Conv2d-1212            [-1, 256, 8, 8]          33,024\n",
      "    BatchNorm2d-1213            [-1, 256, 8, 8]             512\n",
      "           ReLU-1214            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-1215            [-1, 256, 8, 8]               0\n",
      "         Conv2d-1216            [-1, 128, 8, 8]          32,896\n",
      "    BatchNorm2d-1217            [-1, 128, 8, 8]             256\n",
      "           ReLU-1218            [-1, 128, 8, 8]               0\n",
      "         Linear-1219             [-1, 8, 8, 16]             256\n",
      "         Linear-1220             [-1, 8, 8, 16]             256\n",
      "         Linear-1221             [-1, 8, 8, 16]             256\n",
      "         Linear-1222            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1223            [-1, 128, 8, 8]               0\n",
      "         Linear-1224             [-1, 8, 8, 16]             256\n",
      "         Linear-1225             [-1, 8, 8, 16]             256\n",
      "         Linear-1226             [-1, 8, 8, 16]             256\n",
      "         Linear-1227            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1228            [-1, 128, 8, 8]               0\n",
      "           ReLU-1229            [-1, 128, 8, 8]               0\n",
      "         Conv2d-1230            [-1, 256, 8, 8]          33,024\n",
      "    BatchNorm2d-1231            [-1, 256, 8, 8]             512\n",
      "           ReLU-1232            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-1233            [-1, 256, 8, 8]               0\n",
      "        Encoder-1234            [-1, 256, 8, 8]               0\n",
      "         Conv2d-1235            [-1, 256, 8, 8]          37,888\n",
      "         Conv2d-1236            [-1, 128, 8, 8]          32,896\n",
      "    BatchNorm2d-1237            [-1, 128, 8, 8]             256\n",
      "           ReLU-1238            [-1, 128, 8, 8]               0\n",
      "         Linear-1239             [-1, 8, 8, 16]             256\n",
      "         Linear-1240             [-1, 8, 8, 16]             256\n",
      "         Linear-1241             [-1, 8, 8, 16]             256\n",
      "         Linear-1242            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1243            [-1, 128, 8, 8]               0\n",
      "         Linear-1244             [-1, 8, 8, 16]             256\n",
      "         Linear-1245             [-1, 8, 8, 16]             256\n",
      "         Linear-1246             [-1, 8, 8, 16]             256\n",
      "         Linear-1247            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1248            [-1, 128, 8, 8]               0\n",
      "           ReLU-1249            [-1, 128, 8, 8]               0\n",
      "         Conv2d-1250            [-1, 256, 8, 8]          33,024\n",
      "    BatchNorm2d-1251            [-1, 256, 8, 8]             512\n",
      "           ReLU-1252            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-1253            [-1, 256, 8, 8]               0\n",
      "         Conv2d-1254            [-1, 128, 8, 8]          32,896\n",
      "    BatchNorm2d-1255            [-1, 128, 8, 8]             256\n",
      "           ReLU-1256            [-1, 128, 8, 8]               0\n",
      "         Linear-1257             [-1, 8, 8, 16]             256\n",
      "         Linear-1258             [-1, 8, 8, 16]             256\n",
      "         Linear-1259             [-1, 8, 8, 16]             256\n",
      "         Linear-1260            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1261            [-1, 128, 8, 8]               0\n",
      "         Linear-1262             [-1, 8, 8, 16]             256\n",
      "         Linear-1263             [-1, 8, 8, 16]             256\n",
      "         Linear-1264             [-1, 8, 8, 16]             256\n",
      "         Linear-1265            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1266            [-1, 128, 8, 8]               0\n",
      "           ReLU-1267            [-1, 128, 8, 8]               0\n",
      "         Conv2d-1268            [-1, 256, 8, 8]          33,024\n",
      "    BatchNorm2d-1269            [-1, 256, 8, 8]             512\n",
      "           ReLU-1270            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-1271            [-1, 256, 8, 8]               0\n",
      "         Conv2d-1272            [-1, 128, 8, 8]          32,896\n",
      "    BatchNorm2d-1273            [-1, 128, 8, 8]             256\n",
      "           ReLU-1274            [-1, 128, 8, 8]               0\n",
      "         Linear-1275             [-1, 8, 8, 16]             256\n",
      "         Linear-1276             [-1, 8, 8, 16]             256\n",
      "         Linear-1277             [-1, 8, 8, 16]             256\n",
      "         Linear-1278            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1279            [-1, 128, 8, 8]               0\n",
      "         Linear-1280             [-1, 8, 8, 16]             256\n",
      "         Linear-1281             [-1, 8, 8, 16]             256\n",
      "         Linear-1282             [-1, 8, 8, 16]             256\n",
      "         Linear-1283            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1284            [-1, 128, 8, 8]               0\n",
      "           ReLU-1285            [-1, 128, 8, 8]               0\n",
      "         Conv2d-1286            [-1, 256, 8, 8]          33,024\n",
      "    BatchNorm2d-1287            [-1, 256, 8, 8]             512\n",
      "           ReLU-1288            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-1289            [-1, 256, 8, 8]               0\n",
      "         Conv2d-1290            [-1, 128, 8, 8]          32,896\n",
      "    BatchNorm2d-1291            [-1, 128, 8, 8]             256\n",
      "           ReLU-1292            [-1, 128, 8, 8]               0\n",
      "         Linear-1293             [-1, 8, 8, 16]             256\n",
      "         Linear-1294             [-1, 8, 8, 16]             256\n",
      "         Linear-1295             [-1, 8, 8, 16]             256\n",
      "         Linear-1296            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1297            [-1, 128, 8, 8]               0\n",
      "         Linear-1298             [-1, 8, 8, 16]             256\n",
      "         Linear-1299             [-1, 8, 8, 16]             256\n",
      "         Linear-1300             [-1, 8, 8, 16]             256\n",
      "         Linear-1301            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1302            [-1, 128, 8, 8]               0\n",
      "           ReLU-1303            [-1, 128, 8, 8]               0\n",
      "         Conv2d-1304            [-1, 256, 8, 8]          33,024\n",
      "    BatchNorm2d-1305            [-1, 256, 8, 8]             512\n",
      "           ReLU-1306            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-1307            [-1, 256, 8, 8]               0\n",
      "         Conv2d-1308            [-1, 128, 8, 8]          32,896\n",
      "    BatchNorm2d-1309            [-1, 128, 8, 8]             256\n",
      "           ReLU-1310            [-1, 128, 8, 8]               0\n",
      "         Linear-1311             [-1, 8, 8, 16]             256\n",
      "         Linear-1312             [-1, 8, 8, 16]             256\n",
      "         Linear-1313             [-1, 8, 8, 16]             256\n",
      "         Linear-1314            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1315            [-1, 128, 8, 8]               0\n",
      "         Linear-1316             [-1, 8, 8, 16]             256\n",
      "         Linear-1317             [-1, 8, 8, 16]             256\n",
      "         Linear-1318             [-1, 8, 8, 16]             256\n",
      "         Linear-1319            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1320            [-1, 128, 8, 8]               0\n",
      "           ReLU-1321            [-1, 128, 8, 8]               0\n",
      "         Conv2d-1322            [-1, 256, 8, 8]          33,024\n",
      "    BatchNorm2d-1323            [-1, 256, 8, 8]             512\n",
      "           ReLU-1324            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-1325            [-1, 256, 8, 8]               0\n",
      "        Encoder-1326            [-1, 256, 8, 8]               0\n",
      "         Conv2d-1327            [-1, 256, 8, 8]          37,888\n",
      "         Conv2d-1328            [-1, 128, 8, 8]          32,896\n",
      "    BatchNorm2d-1329            [-1, 128, 8, 8]             256\n",
      "           ReLU-1330            [-1, 128, 8, 8]               0\n",
      "         Linear-1331             [-1, 8, 8, 16]             256\n",
      "         Linear-1332             [-1, 8, 8, 16]             256\n",
      "         Linear-1333             [-1, 8, 8, 16]             256\n",
      "         Linear-1334            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1335            [-1, 128, 8, 8]               0\n",
      "         Linear-1336             [-1, 8, 8, 16]             256\n",
      "         Linear-1337             [-1, 8, 8, 16]             256\n",
      "         Linear-1338             [-1, 8, 8, 16]             256\n",
      "         Linear-1339            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1340            [-1, 128, 8, 8]               0\n",
      "           ReLU-1341            [-1, 128, 8, 8]               0\n",
      "         Conv2d-1342            [-1, 256, 8, 8]          33,024\n",
      "    BatchNorm2d-1343            [-1, 256, 8, 8]             512\n",
      "           ReLU-1344            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-1345            [-1, 256, 8, 8]               0\n",
      "         Conv2d-1346            [-1, 128, 8, 8]          32,896\n",
      "    BatchNorm2d-1347            [-1, 128, 8, 8]             256\n",
      "           ReLU-1348            [-1, 128, 8, 8]               0\n",
      "         Linear-1349             [-1, 8, 8, 16]             256\n",
      "         Linear-1350             [-1, 8, 8, 16]             256\n",
      "         Linear-1351             [-1, 8, 8, 16]             256\n",
      "         Linear-1352            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1353            [-1, 128, 8, 8]               0\n",
      "         Linear-1354             [-1, 8, 8, 16]             256\n",
      "         Linear-1355             [-1, 8, 8, 16]             256\n",
      "         Linear-1356             [-1, 8, 8, 16]             256\n",
      "         Linear-1357            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1358            [-1, 128, 8, 8]               0\n",
      "           ReLU-1359            [-1, 128, 8, 8]               0\n",
      "         Conv2d-1360            [-1, 256, 8, 8]          33,024\n",
      "    BatchNorm2d-1361            [-1, 256, 8, 8]             512\n",
      "           ReLU-1362            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-1363            [-1, 256, 8, 8]               0\n",
      "         Conv2d-1364            [-1, 128, 8, 8]          32,896\n",
      "    BatchNorm2d-1365            [-1, 128, 8, 8]             256\n",
      "           ReLU-1366            [-1, 128, 8, 8]               0\n",
      "         Linear-1367             [-1, 8, 8, 16]             256\n",
      "         Linear-1368             [-1, 8, 8, 16]             256\n",
      "         Linear-1369             [-1, 8, 8, 16]             256\n",
      "         Linear-1370            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1371            [-1, 128, 8, 8]               0\n",
      "         Linear-1372             [-1, 8, 8, 16]             256\n",
      "         Linear-1373             [-1, 8, 8, 16]             256\n",
      "         Linear-1374             [-1, 8, 8, 16]             256\n",
      "         Linear-1375            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1376            [-1, 128, 8, 8]               0\n",
      "           ReLU-1377            [-1, 128, 8, 8]               0\n",
      "         Conv2d-1378            [-1, 256, 8, 8]          33,024\n",
      "    BatchNorm2d-1379            [-1, 256, 8, 8]             512\n",
      "           ReLU-1380            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-1381            [-1, 256, 8, 8]               0\n",
      "         Conv2d-1382            [-1, 128, 8, 8]          32,896\n",
      "    BatchNorm2d-1383            [-1, 128, 8, 8]             256\n",
      "           ReLU-1384            [-1, 128, 8, 8]               0\n",
      "         Linear-1385             [-1, 8, 8, 16]             256\n",
      "         Linear-1386             [-1, 8, 8, 16]             256\n",
      "         Linear-1387             [-1, 8, 8, 16]             256\n",
      "         Linear-1388            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1389            [-1, 128, 8, 8]               0\n",
      "         Linear-1390             [-1, 8, 8, 16]             256\n",
      "         Linear-1391             [-1, 8, 8, 16]             256\n",
      "         Linear-1392             [-1, 8, 8, 16]             256\n",
      "         Linear-1393            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1394            [-1, 128, 8, 8]               0\n",
      "           ReLU-1395            [-1, 128, 8, 8]               0\n",
      "         Conv2d-1396            [-1, 256, 8, 8]          33,024\n",
      "    BatchNorm2d-1397            [-1, 256, 8, 8]             512\n",
      "           ReLU-1398            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-1399            [-1, 256, 8, 8]               0\n",
      "         Conv2d-1400            [-1, 128, 8, 8]          32,896\n",
      "    BatchNorm2d-1401            [-1, 128, 8, 8]             256\n",
      "           ReLU-1402            [-1, 128, 8, 8]               0\n",
      "         Linear-1403             [-1, 8, 8, 16]             256\n",
      "         Linear-1404             [-1, 8, 8, 16]             256\n",
      "         Linear-1405             [-1, 8, 8, 16]             256\n",
      "         Linear-1406            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1407            [-1, 128, 8, 8]               0\n",
      "         Linear-1408             [-1, 8, 8, 16]             256\n",
      "         Linear-1409             [-1, 8, 8, 16]             256\n",
      "         Linear-1410             [-1, 8, 8, 16]             256\n",
      "         Linear-1411            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1412            [-1, 128, 8, 8]               0\n",
      "           ReLU-1413            [-1, 128, 8, 8]               0\n",
      "         Conv2d-1414            [-1, 256, 8, 8]          33,024\n",
      "    BatchNorm2d-1415            [-1, 256, 8, 8]             512\n",
      "           ReLU-1416            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-1417            [-1, 256, 8, 8]               0\n",
      "        Encoder-1418            [-1, 256, 8, 8]               0\n",
      "         Conv2d-1419            [-1, 256, 8, 8]          37,888\n",
      "         Conv2d-1420            [-1, 128, 8, 8]          32,896\n",
      "    BatchNorm2d-1421            [-1, 128, 8, 8]             256\n",
      "           ReLU-1422            [-1, 128, 8, 8]               0\n",
      "         Linear-1423             [-1, 8, 8, 16]             256\n",
      "         Linear-1424             [-1, 8, 8, 16]             256\n",
      "         Linear-1425             [-1, 8, 8, 16]             256\n",
      "         Linear-1426            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1427            [-1, 128, 8, 8]               0\n",
      "         Linear-1428             [-1, 8, 8, 16]             256\n",
      "         Linear-1429             [-1, 8, 8, 16]             256\n",
      "         Linear-1430             [-1, 8, 8, 16]             256\n",
      "         Linear-1431            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1432            [-1, 128, 8, 8]               0\n",
      "           ReLU-1433            [-1, 128, 8, 8]               0\n",
      "         Conv2d-1434            [-1, 256, 8, 8]          33,024\n",
      "    BatchNorm2d-1435            [-1, 256, 8, 8]             512\n",
      "           ReLU-1436            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-1437            [-1, 256, 8, 8]               0\n",
      "         Conv2d-1438            [-1, 128, 8, 8]          32,896\n",
      "    BatchNorm2d-1439            [-1, 128, 8, 8]             256\n",
      "           ReLU-1440            [-1, 128, 8, 8]               0\n",
      "         Linear-1441             [-1, 8, 8, 16]             256\n",
      "         Linear-1442             [-1, 8, 8, 16]             256\n",
      "         Linear-1443             [-1, 8, 8, 16]             256\n",
      "         Linear-1444            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1445            [-1, 128, 8, 8]               0\n",
      "         Linear-1446             [-1, 8, 8, 16]             256\n",
      "         Linear-1447             [-1, 8, 8, 16]             256\n",
      "         Linear-1448             [-1, 8, 8, 16]             256\n",
      "         Linear-1449            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1450            [-1, 128, 8, 8]               0\n",
      "           ReLU-1451            [-1, 128, 8, 8]               0\n",
      "         Conv2d-1452            [-1, 256, 8, 8]          33,024\n",
      "    BatchNorm2d-1453            [-1, 256, 8, 8]             512\n",
      "           ReLU-1454            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-1455            [-1, 256, 8, 8]               0\n",
      "         Conv2d-1456            [-1, 128, 8, 8]          32,896\n",
      "    BatchNorm2d-1457            [-1, 128, 8, 8]             256\n",
      "           ReLU-1458            [-1, 128, 8, 8]               0\n",
      "         Linear-1459             [-1, 8, 8, 16]             256\n",
      "         Linear-1460             [-1, 8, 8, 16]             256\n",
      "         Linear-1461             [-1, 8, 8, 16]             256\n",
      "         Linear-1462            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1463            [-1, 128, 8, 8]               0\n",
      "         Linear-1464             [-1, 8, 8, 16]             256\n",
      "         Linear-1465             [-1, 8, 8, 16]             256\n",
      "         Linear-1466             [-1, 8, 8, 16]             256\n",
      "         Linear-1467            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1468            [-1, 128, 8, 8]               0\n",
      "           ReLU-1469            [-1, 128, 8, 8]               0\n",
      "         Conv2d-1470            [-1, 256, 8, 8]          33,024\n",
      "    BatchNorm2d-1471            [-1, 256, 8, 8]             512\n",
      "           ReLU-1472            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-1473            [-1, 256, 8, 8]               0\n",
      "         Conv2d-1474            [-1, 128, 8, 8]          32,896\n",
      "    BatchNorm2d-1475            [-1, 128, 8, 8]             256\n",
      "           ReLU-1476            [-1, 128, 8, 8]               0\n",
      "         Linear-1477             [-1, 8, 8, 16]             256\n",
      "         Linear-1478             [-1, 8, 8, 16]             256\n",
      "         Linear-1479             [-1, 8, 8, 16]             256\n",
      "         Linear-1480            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1481            [-1, 128, 8, 8]               0\n",
      "         Linear-1482             [-1, 8, 8, 16]             256\n",
      "         Linear-1483             [-1, 8, 8, 16]             256\n",
      "         Linear-1484             [-1, 8, 8, 16]             256\n",
      "         Linear-1485            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1486            [-1, 128, 8, 8]               0\n",
      "           ReLU-1487            [-1, 128, 8, 8]               0\n",
      "         Conv2d-1488            [-1, 256, 8, 8]          33,024\n",
      "    BatchNorm2d-1489            [-1, 256, 8, 8]             512\n",
      "           ReLU-1490            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-1491            [-1, 256, 8, 8]               0\n",
      "         Conv2d-1492            [-1, 128, 8, 8]          32,896\n",
      "    BatchNorm2d-1493            [-1, 128, 8, 8]             256\n",
      "           ReLU-1494            [-1, 128, 8, 8]               0\n",
      "         Linear-1495             [-1, 8, 8, 16]             256\n",
      "         Linear-1496             [-1, 8, 8, 16]             256\n",
      "         Linear-1497             [-1, 8, 8, 16]             256\n",
      "         Linear-1498            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1499            [-1, 128, 8, 8]               0\n",
      "         Linear-1500             [-1, 8, 8, 16]             256\n",
      "         Linear-1501             [-1, 8, 8, 16]             256\n",
      "         Linear-1502             [-1, 8, 8, 16]             256\n",
      "         Linear-1503            [-1, 8, 8, 128]          16,512\n",
      " AxialAttention-1504            [-1, 128, 8, 8]               0\n",
      "           ReLU-1505            [-1, 128, 8, 8]               0\n",
      "         Conv2d-1506            [-1, 256, 8, 8]          33,024\n",
      "    BatchNorm2d-1507            [-1, 256, 8, 8]             512\n",
      "           ReLU-1508            [-1, 256, 8, 8]               0\n",
      "GatedAxialTransformerLayer-1509            [-1, 256, 8, 8]               0\n",
      "        Encoder-1510            [-1, 256, 8, 8]               0\n",
      "AdaptiveAvgPool2d-1511            [-1, 256, 1, 1]               0\n",
      "         Linear-1512                   [-1, 10]           2,570\n",
      "        Decoder-1513                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 8,949,002\n",
      "Trainable params: 8,949,002\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 124.13\n",
      "Params size (MB): 34.14\n",
      "Estimated Total Size (MB): 158.28\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "summary(model.cuda(), (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e9b7d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a277c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
