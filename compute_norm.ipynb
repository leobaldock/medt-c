{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Tuple\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.distributed import Backend\n",
    "from torch.nn.parallel.distributed import DistributedDataParallel\n",
    "from torch.utils.data import Dataset, DataLoader, DistributedSampler\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "import nibabel as nib\n",
    "from training import Trainer\n",
    "from notionIntegration import tqdm_notion\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from skimage.transform import resize\n",
    "\n",
    "from MedT_C import MedT_C\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "METADATA_FILENAME = \"meta_data_with_label.json\"\n",
    "\n",
    "class ADNI(Dataset):\n",
    "    \"\"\"ADNI Dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root, transform=None):\n",
    "        self._transform = transform\n",
    "        self._root = root\n",
    "\n",
    "        # Load information from metadata file.\n",
    "        metadata_file = open(os.path.join(self._root, METADATA_FILENAME))\n",
    "        self._metadata = json.load(metadata_file)\n",
    "        self._data = next(os.walk(os.path.join(root, \"images\")), (None, None, []))[2]  # [] if no file\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._metadata)*16 # Because we took 16 slices.\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.toList()\n",
    "\n",
    "        filename = self._data[idx]\n",
    "        nifti_image = nib.load(os.path.join(self._root, \"images\", filename))\n",
    "        nii_data = nifti_image.get_fdata()\n",
    "\n",
    "        if self._transform:\n",
    "            nii_data = self._transform(nii_data)\n",
    "\n",
    "        label = self._metadata[filename.split(\"_\")[0]][\"label\"]\n",
    "\n",
    "        return (nii_data, label)\n",
    "\n",
    "    class Resize(object):\n",
    "        def __init__(self, output_size):\n",
    "            assert isinstance(output_size, (int, tuple))\n",
    "            if isinstance(output_size, int):\n",
    "                self.output_size = (output_size, output_size)\n",
    "            else:\n",
    "                assert len(output_size) == 2\n",
    "                self.output_size = output_size\n",
    "        \n",
    "        def __call__(self, sample):\n",
    "            return resize(sample, self.output_size)\n",
    "    \n",
    "    class Normalize(object):        \n",
    "        def __call__(self, sample):\n",
    "            return sample*255.0/sample.max()\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        ADNI.Normalize(),\n",
    "        ADNI.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize((91.8696,), (495.5406,))\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = ADNI(\"./data/ADNI_sliced\", transform=transform)\n",
    "\n",
    "# img = dataset.__getitem__(50)[0]\n",
    "# plt.imshow(img)\n",
    "\n",
    "image_loader = DataLoader(dataset = dataset, \n",
    "                          batch_size = 64, \n",
    "                          shuffle = False, \n",
    "                          num_workers = 8,\n",
    "                          pin_memory = True)\n",
    "\n",
    "# placeholders\n",
    "psum = torch.tensor([0.0])\n",
    "psum_sq = torch.tensor([0.0])\n",
    "\n",
    "# loop through images\n",
    "for data, labels in tqdm(image_loader):\n",
    "    # break\n",
    "    psum += data.sum(axis = [0, 2, 3])\n",
    "    psum_sq += (data ** 2).sum(axis = [0, 2, 3])\n",
    "\n",
    "# (images, labels) = next(iter(image_loader))\n",
    "def show(img):\n",
    "    npimg = img.numpy()\n",
    "    fig = plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
    "    fig.axes.get_xaxis().set_visible(False)\n",
    "    fig.axes.get_yaxis().set_visible(False)\n",
    "    plt.show()\n",
    "\n",
    "# show(make_grid(images.cpu().data))\n",
    "\n",
    "# show(make_grid(zeros.cpu().data))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 547/547 [00:16<00:00, 32.59it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "####### FINAL CALCULATIONS\n",
    "\n",
    "# pixel count\n",
    "count = len(dataset) * 224 * 224\n",
    "\n",
    "# mean and std\n",
    "total_mean = psum / count\n",
    "total_var  = (psum_sq / count) - (total_mean ** 2)\n",
    "total_std  = torch.sqrt(total_var)\n",
    "\n",
    "# output\n",
    "print('mean: '  + str(total_mean))\n",
    "print('std:  '  + str(total_std))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mean: tensor([27.0715])\n",
      "std:  tensor([53.3210])\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('medt-c': conda)"
  },
  "interpreter": {
   "hash": "12cd09a0c222e58c34f53359e41414927e0c54f1a8e49f2ecf04a2cb7d924487"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}